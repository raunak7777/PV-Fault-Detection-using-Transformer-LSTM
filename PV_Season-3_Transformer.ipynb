{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76ea5733",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, roc_curve, precision_recall_curve, accuracy_score, classification_report, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "332afc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r'C:\\Users\\My PC\\Desktop\\Solar PV Fault Research\\Seasonal_Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f25949bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Season-3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1aaad906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Inv</th>\n",
       "      <th>AC_Real_Power</th>\n",
       "      <th>AC_Current</th>\n",
       "      <th>DC_Power</th>\n",
       "      <th>DC_Current</th>\n",
       "      <th>Tilt_Irradiation_1</th>\n",
       "      <th>Temp_Ambient_1</th>\n",
       "      <th>Temp_Module_1</th>\n",
       "      <th>Wind_Speed_1</th>\n",
       "      <th>Hor_Irradiation_1</th>\n",
       "      <th>Operation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2920</td>\n",
       "      <td>2920</td>\n",
       "      <td>2019-06-01</td>\n",
       "      <td>05:45:00</td>\n",
       "      <td>Inv-1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.3</td>\n",
       "      <td>31.4</td>\n",
       "      <td>2.559</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2921</td>\n",
       "      <td>2921</td>\n",
       "      <td>2019-06-01</td>\n",
       "      <td>06:00:00</td>\n",
       "      <td>Inv-1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.2</td>\n",
       "      <td>28.4</td>\n",
       "      <td>31.5</td>\n",
       "      <td>1.356</td>\n",
       "      <td>10.7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2922</td>\n",
       "      <td>2922</td>\n",
       "      <td>2019-06-01</td>\n",
       "      <td>06:15:00</td>\n",
       "      <td>Inv-1</td>\n",
       "      <td>1.44</td>\n",
       "      <td>6.77</td>\n",
       "      <td>1.77</td>\n",
       "      <td>3.92</td>\n",
       "      <td>26.1</td>\n",
       "      <td>28.7</td>\n",
       "      <td>31.8</td>\n",
       "      <td>1.223</td>\n",
       "      <td>26.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2923</td>\n",
       "      <td>2923</td>\n",
       "      <td>2019-06-01</td>\n",
       "      <td>06:30:00</td>\n",
       "      <td>Inv-1</td>\n",
       "      <td>15.84</td>\n",
       "      <td>29.46</td>\n",
       "      <td>19.18</td>\n",
       "      <td>35.65</td>\n",
       "      <td>47.2</td>\n",
       "      <td>29.1</td>\n",
       "      <td>32.9</td>\n",
       "      <td>1.638</td>\n",
       "      <td>43.8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2924</td>\n",
       "      <td>2924</td>\n",
       "      <td>2019-06-01</td>\n",
       "      <td>06:45:00</td>\n",
       "      <td>Inv-1</td>\n",
       "      <td>29.47</td>\n",
       "      <td>54.35</td>\n",
       "      <td>34.78</td>\n",
       "      <td>61.22</td>\n",
       "      <td>68.0</td>\n",
       "      <td>29.7</td>\n",
       "      <td>34.1</td>\n",
       "      <td>1.797</td>\n",
       "      <td>60.8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69409</th>\n",
       "      <td>6936</td>\n",
       "      <td>6936</td>\n",
       "      <td>2019-08-31</td>\n",
       "      <td>18:30:00</td>\n",
       "      <td>Inv-16</td>\n",
       "      <td>19.12</td>\n",
       "      <td>34.96</td>\n",
       "      <td>23.02</td>\n",
       "      <td>40.54</td>\n",
       "      <td>19.8</td>\n",
       "      <td>29.7</td>\n",
       "      <td>34.9</td>\n",
       "      <td>0.569</td>\n",
       "      <td>27.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69410</th>\n",
       "      <td>6937</td>\n",
       "      <td>6937</td>\n",
       "      <td>2019-08-31</td>\n",
       "      <td>18:45:00</td>\n",
       "      <td>Inv-16</td>\n",
       "      <td>6.54</td>\n",
       "      <td>13.83</td>\n",
       "      <td>8.04</td>\n",
       "      <td>17.45</td>\n",
       "      <td>2.2</td>\n",
       "      <td>29.0</td>\n",
       "      <td>33.5</td>\n",
       "      <td>0.500</td>\n",
       "      <td>7.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69411</th>\n",
       "      <td>6938</td>\n",
       "      <td>6938</td>\n",
       "      <td>2019-08-31</td>\n",
       "      <td>19:00:00</td>\n",
       "      <td>Inv-16</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.4</td>\n",
       "      <td>32.5</td>\n",
       "      <td>0.500</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69412</th>\n",
       "      <td>6939</td>\n",
       "      <td>6939</td>\n",
       "      <td>2019-08-31</td>\n",
       "      <td>19:15:00</td>\n",
       "      <td>Inv-16</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.2</td>\n",
       "      <td>32.4</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69413</th>\n",
       "      <td>6940</td>\n",
       "      <td>6940</td>\n",
       "      <td>2019-08-31</td>\n",
       "      <td>19:30:00</td>\n",
       "      <td>Inv-16</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.1</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69414 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0.1  Unnamed: 0        Date      Time     Inv  AC_Real_Power  \\\n",
       "0              2920        2920  2019-06-01  05:45:00   Inv-1           0.00   \n",
       "1              2921        2921  2019-06-01  06:00:00   Inv-1           0.00   \n",
       "2              2922        2922  2019-06-01  06:15:00   Inv-1           1.44   \n",
       "3              2923        2923  2019-06-01  06:30:00   Inv-1          15.84   \n",
       "4              2924        2924  2019-06-01  06:45:00   Inv-1          29.47   \n",
       "...             ...         ...         ...       ...     ...            ...   \n",
       "69409          6936        6936  2019-08-31  18:30:00  Inv-16          19.12   \n",
       "69410          6937        6937  2019-08-31  18:45:00  Inv-16           6.54   \n",
       "69411          6938        6938  2019-08-31  19:00:00  Inv-16           0.00   \n",
       "69412          6939        6939  2019-08-31  19:15:00  Inv-16           0.00   \n",
       "69413          6940        6940  2019-08-31  19:30:00  Inv-16           0.00   \n",
       "\n",
       "       AC_Current  DC_Power  DC_Current  Tilt_Irradiation_1  Temp_Ambient_1  \\\n",
       "0            0.00      0.00        0.00                 0.0            28.3   \n",
       "1            0.00      0.00        0.00                 8.2            28.4   \n",
       "2            6.77      1.77        3.92                26.1            28.7   \n",
       "3           29.46     19.18       35.65                47.2            29.1   \n",
       "4           54.35     34.78       61.22                68.0            29.7   \n",
       "...           ...       ...         ...                 ...             ...   \n",
       "69409       34.96     23.02       40.54                19.8            29.7   \n",
       "69410       13.83      8.04       17.45                 2.2            29.0   \n",
       "69411        0.00      0.00        0.00                 0.0            28.4   \n",
       "69412        0.00      0.00        0.01                 0.0            28.2   \n",
       "69413        0.00      0.00        2.06                 0.0            28.1   \n",
       "\n",
       "       Temp_Module_1  Wind_Speed_1  Hor_Irradiation_1  Operation  \n",
       "0               31.4         2.559                2.2          1  \n",
       "1               31.5         1.356               10.7          1  \n",
       "2               31.8         1.223               26.2          1  \n",
       "3               32.9         1.638               43.8          1  \n",
       "4               34.1         1.797               60.8          1  \n",
       "...              ...           ...                ...        ...  \n",
       "69409           34.9         0.569               27.6          1  \n",
       "69410           33.5         0.500                7.4          1  \n",
       "69411           32.5         0.500                1.3          1  \n",
       "69412           32.4         0.500                0.2          1  \n",
       "69413           32.0         0.500                0.2          1  \n",
       "\n",
       "[69414 rows x 15 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34b59954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 69414 entries, 0 to 69413\n",
      "Data columns (total 15 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   Unnamed: 0.1        69414 non-null  int64  \n",
      " 1   Unnamed: 0          69414 non-null  int64  \n",
      " 2   Date                69414 non-null  object \n",
      " 3   Time                69414 non-null  object \n",
      " 4   Inv                 69414 non-null  object \n",
      " 5   AC_Real_Power       69414 non-null  float64\n",
      " 6   AC_Current          69414 non-null  float64\n",
      " 7   DC_Power            69414 non-null  float64\n",
      " 8   DC_Current          69414 non-null  float64\n",
      " 9   Tilt_Irradiation_1  69414 non-null  float64\n",
      " 10  Temp_Ambient_1      69414 non-null  float64\n",
      " 11  Temp_Module_1       69414 non-null  float64\n",
      " 12  Wind_Speed_1        69414 non-null  float64\n",
      " 13  Hor_Irradiation_1   69414 non-null  float64\n",
      " 14  Operation           69414 non-null  int64  \n",
      "dtypes: float64(9), int64(3), object(3)\n",
      "memory usage: 7.9+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Inv</th>\n",
       "      <th>AC_Real_Power</th>\n",
       "      <th>AC_Current</th>\n",
       "      <th>DC_Power</th>\n",
       "      <th>DC_Current</th>\n",
       "      <th>Tilt_Irradiation_1</th>\n",
       "      <th>Temp_Ambient_1</th>\n",
       "      <th>Temp_Module_1</th>\n",
       "      <th>Wind_Speed_1</th>\n",
       "      <th>Hor_Irradiation_1</th>\n",
       "      <th>Operation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2920</td>\n",
       "      <td>2920</td>\n",
       "      <td>2019-06-01</td>\n",
       "      <td>05:45:00</td>\n",
       "      <td>Inv-1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.3</td>\n",
       "      <td>31.4</td>\n",
       "      <td>2.559</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2921</td>\n",
       "      <td>2921</td>\n",
       "      <td>2019-06-01</td>\n",
       "      <td>06:00:00</td>\n",
       "      <td>Inv-1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.2</td>\n",
       "      <td>28.4</td>\n",
       "      <td>31.5</td>\n",
       "      <td>1.356</td>\n",
       "      <td>10.7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2922</td>\n",
       "      <td>2922</td>\n",
       "      <td>2019-06-01</td>\n",
       "      <td>06:15:00</td>\n",
       "      <td>Inv-1</td>\n",
       "      <td>1.44</td>\n",
       "      <td>6.77</td>\n",
       "      <td>1.77</td>\n",
       "      <td>3.92</td>\n",
       "      <td>26.1</td>\n",
       "      <td>28.7</td>\n",
       "      <td>31.8</td>\n",
       "      <td>1.223</td>\n",
       "      <td>26.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2923</td>\n",
       "      <td>2923</td>\n",
       "      <td>2019-06-01</td>\n",
       "      <td>06:30:00</td>\n",
       "      <td>Inv-1</td>\n",
       "      <td>15.84</td>\n",
       "      <td>29.46</td>\n",
       "      <td>19.18</td>\n",
       "      <td>35.65</td>\n",
       "      <td>47.2</td>\n",
       "      <td>29.1</td>\n",
       "      <td>32.9</td>\n",
       "      <td>1.638</td>\n",
       "      <td>43.8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2924</td>\n",
       "      <td>2924</td>\n",
       "      <td>2019-06-01</td>\n",
       "      <td>06:45:00</td>\n",
       "      <td>Inv-1</td>\n",
       "      <td>29.47</td>\n",
       "      <td>54.35</td>\n",
       "      <td>34.78</td>\n",
       "      <td>61.22</td>\n",
       "      <td>68.0</td>\n",
       "      <td>29.7</td>\n",
       "      <td>34.1</td>\n",
       "      <td>1.797</td>\n",
       "      <td>60.8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0        Date      Time    Inv  AC_Real_Power  \\\n",
       "0          2920        2920  2019-06-01  05:45:00  Inv-1           0.00   \n",
       "1          2921        2921  2019-06-01  06:00:00  Inv-1           0.00   \n",
       "2          2922        2922  2019-06-01  06:15:00  Inv-1           1.44   \n",
       "3          2923        2923  2019-06-01  06:30:00  Inv-1          15.84   \n",
       "4          2924        2924  2019-06-01  06:45:00  Inv-1          29.47   \n",
       "\n",
       "   AC_Current  DC_Power  DC_Current  Tilt_Irradiation_1  Temp_Ambient_1  \\\n",
       "0        0.00      0.00        0.00                 0.0            28.3   \n",
       "1        0.00      0.00        0.00                 8.2            28.4   \n",
       "2        6.77      1.77        3.92                26.1            28.7   \n",
       "3       29.46     19.18       35.65                47.2            29.1   \n",
       "4       54.35     34.78       61.22                68.0            29.7   \n",
       "\n",
       "   Temp_Module_1  Wind_Speed_1  Hor_Irradiation_1  Operation  \n",
       "0           31.4         2.559                2.2          1  \n",
       "1           31.5         1.356               10.7          1  \n",
       "2           31.8         1.223               26.2          1  \n",
       "3           32.9         1.638               43.8          1  \n",
       "4           34.1         1.797               60.8          1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df.info(),df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c69d30f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 69414\n",
      "Number of columns: 15\n"
     ]
    }
   ],
   "source": [
    "num_rows, num_columns = df.shape\n",
    "\n",
    "print(\"Number of rows:\", num_rows)\n",
    "print(\"Number of columns:\", num_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2deefd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['Unnamed: 0.1', 'Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79fff73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DateTime'] = pd.to_datetime(df['Date'] + ' ' + df['Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7829ac34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('DateTime', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24860811",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Date','Time'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b942cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = pd.Timestamp(\"07:00:00\")\n",
    "end_time = pd.Timestamp(\"18:30:00\")\n",
    "\n",
    "# Filtering the data to retain only the rows within the operational hours\n",
    "df = df[(df.index.time >= pd.to_datetime('7:00:00').time()) & (df.index.time <= pd.to_datetime('18:30:00').time())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c081660b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Inv</th>\n",
       "      <th>AC_Real_Power</th>\n",
       "      <th>AC_Current</th>\n",
       "      <th>DC_Power</th>\n",
       "      <th>DC_Current</th>\n",
       "      <th>Tilt_Irradiation_1</th>\n",
       "      <th>Temp_Ambient_1</th>\n",
       "      <th>Temp_Module_1</th>\n",
       "      <th>Wind_Speed_1</th>\n",
       "      <th>Hor_Irradiation_1</th>\n",
       "      <th>Operation</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DateTime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-06-01 07:00:00</th>\n",
       "      <td>Inv-1</td>\n",
       "      <td>46.52</td>\n",
       "      <td>86.12</td>\n",
       "      <td>53.23</td>\n",
       "      <td>95.12</td>\n",
       "      <td>88.6</td>\n",
       "      <td>30.0</td>\n",
       "      <td>34.5</td>\n",
       "      <td>2.846</td>\n",
       "      <td>77.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-01 07:15:00</th>\n",
       "      <td>Inv-1</td>\n",
       "      <td>67.31</td>\n",
       "      <td>123.41</td>\n",
       "      <td>74.43</td>\n",
       "      <td>135.07</td>\n",
       "      <td>109.0</td>\n",
       "      <td>30.8</td>\n",
       "      <td>34.7</td>\n",
       "      <td>1.752</td>\n",
       "      <td>94.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-01 07:30:00</th>\n",
       "      <td>Inv-1</td>\n",
       "      <td>93.54</td>\n",
       "      <td>170.88</td>\n",
       "      <td>101.75</td>\n",
       "      <td>175.83</td>\n",
       "      <td>235.4</td>\n",
       "      <td>32.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>3.925</td>\n",
       "      <td>181.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-01 07:45:00</th>\n",
       "      <td>Inv-1</td>\n",
       "      <td>120.67</td>\n",
       "      <td>221.96</td>\n",
       "      <td>129.63</td>\n",
       "      <td>212.95</td>\n",
       "      <td>295.2</td>\n",
       "      <td>33.0</td>\n",
       "      <td>37.7</td>\n",
       "      <td>2.579</td>\n",
       "      <td>232.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-01 08:00:00</th>\n",
       "      <td>Inv-1</td>\n",
       "      <td>153.92</td>\n",
       "      <td>279.81</td>\n",
       "      <td>163.37</td>\n",
       "      <td>269.89</td>\n",
       "      <td>351.2</td>\n",
       "      <td>33.3</td>\n",
       "      <td>38.8</td>\n",
       "      <td>2.614</td>\n",
       "      <td>282.7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Inv  AC_Real_Power  AC_Current  DC_Power  DC_Current  \\\n",
       "DateTime                                                                      \n",
       "2019-06-01 07:00:00  Inv-1          46.52       86.12     53.23       95.12   \n",
       "2019-06-01 07:15:00  Inv-1          67.31      123.41     74.43      135.07   \n",
       "2019-06-01 07:30:00  Inv-1          93.54      170.88    101.75      175.83   \n",
       "2019-06-01 07:45:00  Inv-1         120.67      221.96    129.63      212.95   \n",
       "2019-06-01 08:00:00  Inv-1         153.92      279.81    163.37      269.89   \n",
       "\n",
       "                     Tilt_Irradiation_1  Temp_Ambient_1  Temp_Module_1  \\\n",
       "DateTime                                                                 \n",
       "2019-06-01 07:00:00                88.6            30.0           34.5   \n",
       "2019-06-01 07:15:00               109.0            30.8           34.7   \n",
       "2019-06-01 07:30:00               235.4            32.0           36.0   \n",
       "2019-06-01 07:45:00               295.2            33.0           37.7   \n",
       "2019-06-01 08:00:00               351.2            33.3           38.8   \n",
       "\n",
       "                     Wind_Speed_1  Hor_Irradiation_1  Operation  \n",
       "DateTime                                                         \n",
       "2019-06-01 07:00:00         2.846               77.4          1  \n",
       "2019-06-01 07:15:00         1.752               94.2          1  \n",
       "2019-06-01 07:30:00         3.925              181.2          1  \n",
       "2019-06-01 07:45:00         2.579              232.2          1  \n",
       "2019-06-01 08:00:00         2.614              282.7          1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1088275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inv                   0\n",
      "AC_Real_Power         0\n",
      "AC_Current            0\n",
      "DC_Power              0\n",
      "DC_Current            0\n",
      "Tilt_Irradiation_1    0\n",
      "Temp_Ambient_1        0\n",
      "Temp_Module_1         0\n",
      "Wind_Speed_1          0\n",
      "Hor_Irradiation_1     0\n",
      "Operation             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "missing_data = df.isnull().sum()\n",
    "print(missing_data)\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66f56297",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "operational_data = df\n",
    "numerical_features = ['AC_Real_Power','Tilt_Irradiation_1', 'Temp_Ambient_1', 'Hor_Irradiation_1']\n",
    "\n",
    "# Normalization of the features\n",
    "operational_data[numerical_features] = scaler.fit_transform(operational_data[numerical_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea87dce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = operational_data[['Tilt_Irradiation_1', 'Temp_Ambient_1', 'Hor_Irradiation_1']]\n",
    "y = operational_data['AC_Real_Power']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eefb5c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training - Epoch [1/200], Loss: 0.0459\n",
      "Validation - Epoch [1/200], Loss: 0.0151\n",
      "Training - Epoch [2/200], Loss: 0.0162\n",
      "Validation - Epoch [2/200], Loss: 0.0146\n",
      "Training - Epoch [3/200], Loss: 0.0161\n",
      "Validation - Epoch [3/200], Loss: 0.0159\n",
      "Training - Epoch [4/200], Loss: 0.0159\n",
      "Validation - Epoch [4/200], Loss: 0.0149\n",
      "Training - Epoch [5/200], Loss: 0.0158\n",
      "Validation - Epoch [5/200], Loss: 0.0166\n",
      "Training - Epoch [6/200], Loss: 0.0156\n",
      "Validation - Epoch [6/200], Loss: 0.0175\n",
      "Training - Epoch [7/200], Loss: 0.0158\n",
      "Validation - Epoch [7/200], Loss: 0.0157\n",
      "Training - Epoch [8/200], Loss: 0.0154\n",
      "Validation - Epoch [8/200], Loss: 0.0146\n",
      "Training - Epoch [9/200], Loss: 0.0164\n",
      "Validation - Epoch [9/200], Loss: 0.0161\n",
      "Training - Epoch [10/200], Loss: 0.0161\n",
      "Validation - Epoch [10/200], Loss: 0.0156\n",
      "Training - Epoch [11/200], Loss: 0.0159\n",
      "Validation - Epoch [11/200], Loss: 0.0152\n",
      "Training - Epoch [12/200], Loss: 0.0160\n",
      "Validation - Epoch [12/200], Loss: 0.0182\n",
      "Training - Epoch [13/200], Loss: 0.0159\n",
      "Validation - Epoch [13/200], Loss: 0.0153\n",
      "Training - Epoch [14/200], Loss: 0.0158\n",
      "Validation - Epoch [14/200], Loss: 0.0162\n",
      "Training - Epoch [15/200], Loss: 0.0159\n",
      "Validation - Epoch [15/200], Loss: 0.0151\n",
      "Training - Epoch [16/200], Loss: 0.0156\n",
      "Validation - Epoch [16/200], Loss: 0.0154\n",
      "Training - Epoch [17/200], Loss: 0.0157\n",
      "Validation - Epoch [17/200], Loss: 0.0153\n",
      "Training - Epoch [18/200], Loss: 0.0159\n",
      "Validation - Epoch [18/200], Loss: 0.0149\n",
      "Training - Epoch [19/200], Loss: 0.0157\n",
      "Validation - Epoch [19/200], Loss: 0.0162\n",
      "Training - Epoch [20/200], Loss: 0.0155\n",
      "Validation - Epoch [20/200], Loss: 0.0150\n",
      "Training - Epoch [21/200], Loss: 0.0155\n",
      "Validation - Epoch [21/200], Loss: 0.0154\n",
      "Training - Epoch [22/200], Loss: 0.0155\n",
      "Validation - Epoch [22/200], Loss: 0.0159\n",
      "Training - Epoch [23/200], Loss: 0.0155\n",
      "Validation - Epoch [23/200], Loss: 0.0158\n",
      "Training - Epoch [24/200], Loss: 0.0155\n",
      "Validation - Epoch [24/200], Loss: 0.0153\n",
      "Training - Epoch [25/200], Loss: 0.0154\n",
      "Validation - Epoch [25/200], Loss: 0.0156\n",
      "Training - Epoch [26/200], Loss: 0.0156\n",
      "Validation - Epoch [26/200], Loss: 0.0160\n",
      "Training - Epoch [27/200], Loss: 0.0153\n",
      "Validation - Epoch [27/200], Loss: 0.0149\n",
      "Training - Epoch [28/200], Loss: 0.0153\n",
      "Validation - Epoch [28/200], Loss: 0.0159\n",
      "Training - Epoch [29/200], Loss: 0.0153\n",
      "Validation - Epoch [29/200], Loss: 0.0150\n",
      "Training - Epoch [30/200], Loss: 0.0156\n",
      "Validation - Epoch [30/200], Loss: 0.0151\n",
      "Training - Epoch [31/200], Loss: 0.0153\n",
      "Validation - Epoch [31/200], Loss: 0.0149\n",
      "Training - Epoch [32/200], Loss: 0.0153\n",
      "Validation - Epoch [32/200], Loss: 0.0148\n",
      "Training - Epoch [33/200], Loss: 0.0154\n",
      "Validation - Epoch [33/200], Loss: 0.0153\n",
      "Training - Epoch [34/200], Loss: 0.0154\n",
      "Validation - Epoch [34/200], Loss: 0.0149\n",
      "Training - Epoch [35/200], Loss: 0.0154\n",
      "Validation - Epoch [35/200], Loss: 0.0150\n",
      "Training - Epoch [36/200], Loss: 0.0152\n",
      "Validation - Epoch [36/200], Loss: 0.0157\n",
      "Training - Epoch [37/200], Loss: 0.0152\n",
      "Validation - Epoch [37/200], Loss: 0.0150\n",
      "Training - Epoch [38/200], Loss: 0.0153\n",
      "Validation - Epoch [38/200], Loss: 0.0152\n",
      "Training - Epoch [39/200], Loss: 0.0152\n",
      "Validation - Epoch [39/200], Loss: 0.0154\n",
      "Training - Epoch [40/200], Loss: 0.0154\n",
      "Validation - Epoch [40/200], Loss: 0.0154\n",
      "Training - Epoch [41/200], Loss: 0.0152\n",
      "Validation - Epoch [41/200], Loss: 0.0149\n",
      "Training - Epoch [42/200], Loss: 0.0152\n",
      "Validation - Epoch [42/200], Loss: 0.0149\n",
      "Training - Epoch [43/200], Loss: 0.0151\n",
      "Validation - Epoch [43/200], Loss: 0.0156\n",
      "Training - Epoch [44/200], Loss: 0.0153\n",
      "Validation - Epoch [44/200], Loss: 0.0149\n",
      "Training - Epoch [45/200], Loss: 0.0153\n",
      "Validation - Epoch [45/200], Loss: 0.0151\n",
      "Training - Epoch [46/200], Loss: 0.0151\n",
      "Validation - Epoch [46/200], Loss: 0.0149\n",
      "Training - Epoch [47/200], Loss: 0.0152\n",
      "Validation - Epoch [47/200], Loss: 0.0147\n",
      "Training - Epoch [48/200], Loss: 0.0153\n",
      "Validation - Epoch [48/200], Loss: 0.0151\n",
      "Training - Epoch [49/200], Loss: 0.0151\n",
      "Validation - Epoch [49/200], Loss: 0.0155\n",
      "Training - Epoch [50/200], Loss: 0.0151\n",
      "Validation - Epoch [50/200], Loss: 0.0156\n",
      "Training - Epoch [51/200], Loss: 0.0152\n",
      "Validation - Epoch [51/200], Loss: 0.0147\n",
      "Training - Epoch [52/200], Loss: 0.0150\n",
      "Validation - Epoch [52/200], Loss: 0.0155\n",
      "Training - Epoch [53/200], Loss: 0.0167\n",
      "Validation - Epoch [53/200], Loss: 0.0152\n",
      "Training - Epoch [54/200], Loss: 0.0158\n",
      "Validation - Epoch [54/200], Loss: 0.0157\n",
      "Training - Epoch [55/200], Loss: 0.0154\n",
      "Validation - Epoch [55/200], Loss: 0.0152\n",
      "Training - Epoch [56/200], Loss: 0.0155\n",
      "Validation - Epoch [56/200], Loss: 0.0150\n",
      "Training - Epoch [57/200], Loss: 0.0152\n",
      "Validation - Epoch [57/200], Loss: 0.0149\n",
      "Training - Epoch [58/200], Loss: 0.0155\n",
      "Validation - Epoch [58/200], Loss: 0.0156\n",
      "Training - Epoch [59/200], Loss: 0.0152\n",
      "Validation - Epoch [59/200], Loss: 0.0149\n",
      "Training - Epoch [60/200], Loss: 0.0154\n",
      "Validation - Epoch [60/200], Loss: 0.0169\n",
      "Training - Epoch [61/200], Loss: 0.0156\n",
      "Validation - Epoch [61/200], Loss: 0.0148\n",
      "Training - Epoch [62/200], Loss: 0.0152\n",
      "Validation - Epoch [62/200], Loss: 0.0152\n",
      "Training - Epoch [63/200], Loss: 0.0152\n",
      "Validation - Epoch [63/200], Loss: 0.0150\n",
      "Training - Epoch [64/200], Loss: 0.0152\n",
      "Validation - Epoch [64/200], Loss: 0.0157\n",
      "Training - Epoch [65/200], Loss: 0.0152\n",
      "Validation - Epoch [65/200], Loss: 0.0151\n",
      "Training - Epoch [66/200], Loss: 0.0152\n",
      "Validation - Epoch [66/200], Loss: 0.0149\n",
      "Training - Epoch [67/200], Loss: 0.0152\n",
      "Validation - Epoch [67/200], Loss: 0.0156\n",
      "Training - Epoch [68/200], Loss: 0.0153\n",
      "Validation - Epoch [68/200], Loss: 0.0157\n",
      "Training - Epoch [69/200], Loss: 0.0152\n",
      "Validation - Epoch [69/200], Loss: 0.0156\n",
      "Training - Epoch [70/200], Loss: 0.0152\n",
      "Validation - Epoch [70/200], Loss: 0.0148\n",
      "Training - Epoch [71/200], Loss: 0.0153\n",
      "Validation - Epoch [71/200], Loss: 0.0152\n",
      "Training - Epoch [72/200], Loss: 0.0151\n",
      "Validation - Epoch [72/200], Loss: 0.0151\n",
      "Training - Epoch [73/200], Loss: 0.0153\n",
      "Validation - Epoch [73/200], Loss: 0.0153\n",
      "Training - Epoch [74/200], Loss: 0.0161\n",
      "Validation - Epoch [74/200], Loss: 0.0156\n",
      "Training - Epoch [75/200], Loss: 0.0158\n",
      "Validation - Epoch [75/200], Loss: 0.0156\n",
      "Training - Epoch [76/200], Loss: 0.0154\n",
      "Validation - Epoch [76/200], Loss: 0.0151\n",
      "Training - Epoch [77/200], Loss: 0.0152\n",
      "Validation - Epoch [77/200], Loss: 0.0152\n",
      "Training - Epoch [78/200], Loss: 0.0152\n",
      "Validation - Epoch [78/200], Loss: 0.0153\n",
      "Training - Epoch [79/200], Loss: 0.0152\n",
      "Validation - Epoch [79/200], Loss: 0.0152\n",
      "Training - Epoch [80/200], Loss: 0.0152\n",
      "Validation - Epoch [80/200], Loss: 0.0150\n",
      "Training - Epoch [81/200], Loss: 0.0152\n",
      "Validation - Epoch [81/200], Loss: 0.0159\n",
      "Training - Epoch [82/200], Loss: 0.0158\n",
      "Validation - Epoch [82/200], Loss: 0.0159\n",
      "Training - Epoch [83/200], Loss: 0.0154\n",
      "Validation - Epoch [83/200], Loss: 0.0150\n",
      "Training - Epoch [84/200], Loss: 0.0152\n",
      "Validation - Epoch [84/200], Loss: 0.0158\n",
      "Training - Epoch [85/200], Loss: 0.0154\n",
      "Validation - Epoch [85/200], Loss: 0.0150\n",
      "Training - Epoch [86/200], Loss: 0.0152\n",
      "Validation - Epoch [86/200], Loss: 0.0153\n",
      "Training - Epoch [87/200], Loss: 0.0151\n",
      "Validation - Epoch [87/200], Loss: 0.0149\n",
      "Training - Epoch [88/200], Loss: 0.0151\n",
      "Validation - Epoch [88/200], Loss: 0.0149\n",
      "Training - Epoch [89/200], Loss: 0.0153\n",
      "Validation - Epoch [89/200], Loss: 0.0155\n",
      "Training - Epoch [90/200], Loss: 0.0154\n",
      "Validation - Epoch [90/200], Loss: 0.0161\n",
      "Training - Epoch [91/200], Loss: 0.0152\n",
      "Validation - Epoch [91/200], Loss: 0.0149\n",
      "Training - Epoch [92/200], Loss: 0.0151\n",
      "Validation - Epoch [92/200], Loss: 0.0148\n",
      "Training - Epoch [93/200], Loss: 0.0151\n",
      "Validation - Epoch [93/200], Loss: 0.0151\n",
      "Training - Epoch [94/200], Loss: 0.0153\n",
      "Validation - Epoch [94/200], Loss: 0.0154\n",
      "Training - Epoch [95/200], Loss: 0.0152\n",
      "Validation - Epoch [95/200], Loss: 0.0153\n",
      "Training - Epoch [96/200], Loss: 0.0151\n",
      "Validation - Epoch [96/200], Loss: 0.0158\n",
      "Training - Epoch [97/200], Loss: 0.0152\n",
      "Validation - Epoch [97/200], Loss: 0.0157\n",
      "Training - Epoch [98/200], Loss: 0.0151\n",
      "Validation - Epoch [98/200], Loss: 0.0149\n",
      "Training - Epoch [99/200], Loss: 0.0151\n",
      "Validation - Epoch [99/200], Loss: 0.0148\n",
      "Training - Epoch [100/200], Loss: 0.0151\n",
      "Validation - Epoch [100/200], Loss: 0.0152\n",
      "Training - Epoch [101/200], Loss: 0.0152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Epoch [101/200], Loss: 0.0150\n",
      "Training - Epoch [102/200], Loss: 0.0151\n",
      "Validation - Epoch [102/200], Loss: 0.0154\n",
      "Training - Epoch [103/200], Loss: 0.0151\n",
      "Validation - Epoch [103/200], Loss: 0.0150\n",
      "Training - Epoch [104/200], Loss: 0.0151\n",
      "Validation - Epoch [104/200], Loss: 0.0150\n",
      "Training - Epoch [105/200], Loss: 0.0152\n",
      "Validation - Epoch [105/200], Loss: 0.0179\n",
      "Training - Epoch [106/200], Loss: 0.0159\n",
      "Validation - Epoch [106/200], Loss: 0.0150\n",
      "Training - Epoch [107/200], Loss: 0.0152\n",
      "Validation - Epoch [107/200], Loss: 0.0150\n",
      "Training - Epoch [108/200], Loss: 0.0152\n",
      "Validation - Epoch [108/200], Loss: 0.0162\n",
      "Training - Epoch [109/200], Loss: 0.0151\n",
      "Validation - Epoch [109/200], Loss: 0.0150\n",
      "Training - Epoch [110/200], Loss: 0.0151\n",
      "Validation - Epoch [110/200], Loss: 0.0151\n",
      "Training - Epoch [111/200], Loss: 0.0150\n",
      "Validation - Epoch [111/200], Loss: 0.0151\n",
      "Training - Epoch [112/200], Loss: 0.0154\n",
      "Validation - Epoch [112/200], Loss: 0.0154\n",
      "Training - Epoch [113/200], Loss: 0.0153\n",
      "Validation - Epoch [113/200], Loss: 0.0153\n",
      "Training - Epoch [114/200], Loss: 0.0153\n",
      "Validation - Epoch [114/200], Loss: 0.0148\n",
      "Training - Epoch [115/200], Loss: 0.0151\n",
      "Validation - Epoch [115/200], Loss: 0.0153\n",
      "Training - Epoch [116/200], Loss: 0.0150\n",
      "Validation - Epoch [116/200], Loss: 0.0147\n",
      "Training - Epoch [117/200], Loss: 0.0152\n",
      "Validation - Epoch [117/200], Loss: 0.0151\n",
      "Training - Epoch [118/200], Loss: 0.0151\n",
      "Validation - Epoch [118/200], Loss: 0.0151\n",
      "Training - Epoch [119/200], Loss: 0.0150\n",
      "Validation - Epoch [119/200], Loss: 0.0151\n",
      "Training - Epoch [120/200], Loss: 0.0151\n",
      "Validation - Epoch [120/200], Loss: 0.0151\n",
      "Training - Epoch [121/200], Loss: 0.0153\n",
      "Validation - Epoch [121/200], Loss: 0.0151\n",
      "Training - Epoch [122/200], Loss: 0.0153\n",
      "Validation - Epoch [122/200], Loss: 0.0149\n",
      "Training - Epoch [123/200], Loss: 0.0150\n",
      "Validation - Epoch [123/200], Loss: 0.0151\n",
      "Training - Epoch [124/200], Loss: 0.0152\n",
      "Validation - Epoch [124/200], Loss: 0.0151\n",
      "Training - Epoch [125/200], Loss: 0.0151\n",
      "Validation - Epoch [125/200], Loss: 0.0149\n",
      "Training - Epoch [126/200], Loss: 0.0156\n",
      "Validation - Epoch [126/200], Loss: 0.0172\n",
      "Training - Epoch [127/200], Loss: 0.0154\n",
      "Validation - Epoch [127/200], Loss: 0.0150\n",
      "Training - Epoch [128/200], Loss: 0.0152\n",
      "Validation - Epoch [128/200], Loss: 0.0149\n",
      "Training - Epoch [129/200], Loss: 0.0158\n",
      "Validation - Epoch [129/200], Loss: 0.0157\n",
      "Training - Epoch [130/200], Loss: 0.0156\n",
      "Validation - Epoch [130/200], Loss: 0.0153\n",
      "Training - Epoch [131/200], Loss: 0.0153\n",
      "Validation - Epoch [131/200], Loss: 0.0151\n",
      "Training - Epoch [132/200], Loss: 0.0151\n",
      "Validation - Epoch [132/200], Loss: 0.0151\n",
      "Training - Epoch [133/200], Loss: 0.0151\n",
      "Validation - Epoch [133/200], Loss: 0.0153\n",
      "Training - Epoch [134/200], Loss: 0.0151\n",
      "Validation - Epoch [134/200], Loss: 0.0151\n",
      "Training - Epoch [135/200], Loss: 0.0153\n",
      "Validation - Epoch [135/200], Loss: 0.0160\n",
      "Training - Epoch [136/200], Loss: 0.0152\n",
      "Validation - Epoch [136/200], Loss: 0.0154\n",
      "Training - Epoch [137/200], Loss: 0.0152\n",
      "Validation - Epoch [137/200], Loss: 0.0159\n",
      "Training - Epoch [138/200], Loss: 0.0151\n",
      "Validation - Epoch [138/200], Loss: 0.0160\n",
      "Training - Epoch [139/200], Loss: 0.0151\n",
      "Validation - Epoch [139/200], Loss: 0.0164\n",
      "Training - Epoch [140/200], Loss: 0.0153\n",
      "Validation - Epoch [140/200], Loss: 0.0149\n",
      "Training - Epoch [141/200], Loss: 0.0151\n",
      "Validation - Epoch [141/200], Loss: 0.0147\n",
      "Training - Epoch [142/200], Loss: 0.0152\n",
      "Validation - Epoch [142/200], Loss: 0.0159\n",
      "Training - Epoch [143/200], Loss: 0.0152\n",
      "Validation - Epoch [143/200], Loss: 0.0147\n",
      "Training - Epoch [144/200], Loss: 0.0152\n",
      "Validation - Epoch [144/200], Loss: 0.0151\n",
      "Training - Epoch [145/200], Loss: 0.0153\n",
      "Validation - Epoch [145/200], Loss: 0.0155\n",
      "Training - Epoch [146/200], Loss: 0.0151\n",
      "Validation - Epoch [146/200], Loss: 0.0150\n",
      "Training - Epoch [147/200], Loss: 0.0153\n",
      "Validation - Epoch [147/200], Loss: 0.0151\n",
      "Training - Epoch [148/200], Loss: 0.0152\n",
      "Validation - Epoch [148/200], Loss: 0.0163\n",
      "Training - Epoch [149/200], Loss: 0.0151\n",
      "Validation - Epoch [149/200], Loss: 0.0148\n",
      "Training - Epoch [150/200], Loss: 0.0150\n",
      "Validation - Epoch [150/200], Loss: 0.0152\n",
      "Training - Epoch [151/200], Loss: 0.0151\n",
      "Validation - Epoch [151/200], Loss: 0.0151\n",
      "Training - Epoch [152/200], Loss: 0.0153\n",
      "Validation - Epoch [152/200], Loss: 0.0160\n",
      "Training - Epoch [153/200], Loss: 0.0154\n",
      "Validation - Epoch [153/200], Loss: 0.0148\n",
      "Training - Epoch [154/200], Loss: 0.0151\n",
      "Validation - Epoch [154/200], Loss: 0.0149\n",
      "Training - Epoch [155/200], Loss: 0.0151\n",
      "Validation - Epoch [155/200], Loss: 0.0153\n",
      "Training - Epoch [156/200], Loss: 0.0151\n",
      "Validation - Epoch [156/200], Loss: 0.0150\n",
      "Training - Epoch [157/200], Loss: 0.0153\n",
      "Validation - Epoch [157/200], Loss: 0.0149\n",
      "Training - Epoch [158/200], Loss: 0.0151\n",
      "Validation - Epoch [158/200], Loss: 0.0151\n",
      "Training - Epoch [159/200], Loss: 0.0151\n",
      "Validation - Epoch [159/200], Loss: 0.0150\n",
      "Training - Epoch [160/200], Loss: 0.0156\n",
      "Validation - Epoch [160/200], Loss: 0.0170\n",
      "Training - Epoch [161/200], Loss: 0.0154\n",
      "Validation - Epoch [161/200], Loss: 0.0149\n",
      "Training - Epoch [162/200], Loss: 0.0152\n",
      "Validation - Epoch [162/200], Loss: 0.0152\n",
      "Training - Epoch [163/200], Loss: 0.0153\n",
      "Validation - Epoch [163/200], Loss: 0.0154\n",
      "Training - Epoch [164/200], Loss: 0.0153\n",
      "Validation - Epoch [164/200], Loss: 0.0150\n",
      "Training - Epoch [165/200], Loss: 0.0151\n",
      "Validation - Epoch [165/200], Loss: 0.0148\n",
      "Training - Epoch [166/200], Loss: 0.0155\n",
      "Validation - Epoch [166/200], Loss: 0.0169\n",
      "Training - Epoch [167/200], Loss: 0.0158\n",
      "Validation - Epoch [167/200], Loss: 0.0153\n",
      "Training - Epoch [168/200], Loss: 0.0156\n",
      "Validation - Epoch [168/200], Loss: 0.0156\n",
      "Training - Epoch [169/200], Loss: 0.0153\n",
      "Validation - Epoch [169/200], Loss: 0.0161\n",
      "Training - Epoch [170/200], Loss: 0.0152\n",
      "Validation - Epoch [170/200], Loss: 0.0149\n",
      "Training - Epoch [171/200], Loss: 0.0151\n",
      "Validation - Epoch [171/200], Loss: 0.0148\n",
      "Training - Epoch [172/200], Loss: 0.0151\n",
      "Validation - Epoch [172/200], Loss: 0.0153\n",
      "Training - Epoch [173/200], Loss: 0.0155\n",
      "Validation - Epoch [173/200], Loss: 0.0176\n",
      "Training - Epoch [174/200], Loss: 0.0154\n",
      "Validation - Epoch [174/200], Loss: 0.0158\n",
      "Training - Epoch [175/200], Loss: 0.0152\n",
      "Validation - Epoch [175/200], Loss: 0.0158\n",
      "Training - Epoch [176/200], Loss: 0.0152\n",
      "Validation - Epoch [176/200], Loss: 0.0150\n",
      "Training - Epoch [177/200], Loss: 0.0150\n",
      "Validation - Epoch [177/200], Loss: 0.0148\n",
      "Training - Epoch [178/200], Loss: 0.0150\n",
      "Validation - Epoch [178/200], Loss: 0.0149\n",
      "Training - Epoch [179/200], Loss: 0.0151\n",
      "Validation - Epoch [179/200], Loss: 0.0156\n",
      "Training - Epoch [180/200], Loss: 0.0152\n",
      "Validation - Epoch [180/200], Loss: 0.0150\n",
      "Training - Epoch [181/200], Loss: 0.0151\n",
      "Validation - Epoch [181/200], Loss: 0.0149\n",
      "Training - Epoch [182/200], Loss: 0.0152\n",
      "Validation - Epoch [182/200], Loss: 0.0152\n",
      "Training - Epoch [183/200], Loss: 0.0152\n",
      "Validation - Epoch [183/200], Loss: 0.0149\n",
      "Training - Epoch [184/200], Loss: 0.0152\n",
      "Validation - Epoch [184/200], Loss: 0.0149\n",
      "Training - Epoch [185/200], Loss: 0.0151\n",
      "Validation - Epoch [185/200], Loss: 0.0148\n",
      "Training - Epoch [186/200], Loss: 0.0150\n",
      "Validation - Epoch [186/200], Loss: 0.0154\n",
      "Training - Epoch [187/200], Loss: 0.0152\n",
      "Validation - Epoch [187/200], Loss: 0.0169\n",
      "Training - Epoch [188/200], Loss: 0.0152\n",
      "Validation - Epoch [188/200], Loss: 0.0156\n",
      "Training - Epoch [189/200], Loss: 0.0152\n",
      "Validation - Epoch [189/200], Loss: 0.0148\n",
      "Training - Epoch [190/200], Loss: 0.0150\n",
      "Validation - Epoch [190/200], Loss: 0.0151\n",
      "Training - Epoch [191/200], Loss: 0.0152\n",
      "Validation - Epoch [191/200], Loss: 0.0150\n",
      "Training - Epoch [192/200], Loss: 0.0152\n",
      "Validation - Epoch [192/200], Loss: 0.0156\n",
      "Training - Epoch [193/200], Loss: 0.0151\n",
      "Validation - Epoch [193/200], Loss: 0.0160\n",
      "Training - Epoch [194/200], Loss: 0.0152\n",
      "Validation - Epoch [194/200], Loss: 0.0150\n",
      "Training - Epoch [195/200], Loss: 0.0151\n",
      "Validation - Epoch [195/200], Loss: 0.0149\n",
      "Training - Epoch [196/200], Loss: 0.0151\n",
      "Validation - Epoch [196/200], Loss: 0.0154\n",
      "Training - Epoch [197/200], Loss: 0.0152\n",
      "Validation - Epoch [197/200], Loss: 0.0149\n",
      "Training - Epoch [198/200], Loss: 0.0151\n",
      "Validation - Epoch [198/200], Loss: 0.0154\n",
      "Training - Epoch [199/200], Loss: 0.0151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Epoch [199/200], Loss: 0.0155\n",
      "Training - Epoch [200/200], Loss: 0.0153\n",
      "Validation - Epoch [200/200], Loss: 0.0151\n"
     ]
    }
   ],
   "source": [
    "# Transformer Model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads, hidden_dim, num_layers, dropout):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, hidden_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=num_heads)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.squeeze(1)\n",
    "        x = self.fc(x)\n",
    "        return x.squeeze()\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = torch.tensor(features.values, dtype=torch.float32) \n",
    "        self.targets = torch.tensor(targets.values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.targets[idx]\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Hyperparameters\n",
    "input_dim = X_train.shape[1]  # Dimension of input features\n",
    "num_heads = 2  # Number of attention heads = 2\n",
    "hidden_dim = 200  # Hidden layers of the model = 200\n",
    "num_layers = 2  # Number of transformer layers = 2\n",
    "dropout = 0.1  # Dropout probability = 0.1\n",
    "\n",
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataset = CustomDataset(X_test, y_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "model = TransformerModel(input_dim=input_dim, num_heads=num_heads, hidden_dim=hidden_dim,\n",
    "                          num_layers=num_layers, dropout=dropout)\n",
    "criterion = nn.MSELoss()  # Mean Squared Error loss for regression task\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001) # Optimizer used is Adam and learning rate is 0.001\n",
    "\n",
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, labels in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * len(inputs)\n",
    "    average_loss = total_loss / len(train_dataset)\n",
    "    print(f'Training - Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss:.4f}')\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        for inputs, labels in test_dataloader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item() * len(inputs)\n",
    "        average_loss = total_loss / len(test_dataset)\n",
    "        print(f'Validation - Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss:.4f}')\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_dataloader:\n",
    "        outputs = model(inputs)\n",
    "        # Stored true labels and predictions\n",
    "        y_true.extend(labels.numpy())\n",
    "        y_pred.extend(outputs.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff685a7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqsAAAIhCAYAAABpMPNPAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAz80lEQVR4nO3deVxVdf7H8fdlFxUUEBED11xLQUxH09wtM5NfTVZabrillVvamBmak1s2roj7bppTamrpVC6taFpqWWiZe0KKpCQispzfH453ugEGCvINXs/Hw8d4zzn33M/hMQ96eTjnYLMsyxIAAABgIKfCHgAAAADICbEKAAAAYxGrAAAAMBaxCgAAAGMRqwAAADAWsQoAAABjEasAAAAwFrEKAAAAYxGrAAAAMBaxCsBY33zzjXr16qUqVarIw8NDpUqVUoMGDTRlyhQlJiYW6Gfv27dPLVq0kLe3t2w2m6ZPn57vn2Gz2TR27Nh83++fWbp0qWw2m2w2m3bu3JllvWVZql69umw2m1q2bHlTnzFnzhwtXbo0T+/ZuXNnjjMBKL5cCnsAAMjOggULNHDgQNWsWVMjRoxQnTp1lJaWpr1792ru3LmKiYnR+vXrC+zze/fureTkZK1Zs0Zly5ZV5cqV8/0zYmJidMcdd+T7fnOrdOnSWrRoUZYg/fjjj/XTTz+pdOnSN73vOXPmyM/PTz179sz1exo0aKCYmBjVqVPnpj8XQNFDrAIwTkxMjJ555hm1a9dOGzZskLu7u31du3btNHz4cG3durVAZzh48KD69u2rDh06FNhn/O1vfyuwfefG448/rlWrVikqKkpeXl725YsWLVKTJk2UlJR0W+ZIS0uTzWaTl5dXoX9NAJiHywAAGGfChAmy2WyaP3++Q6he5+bmpocfftj+OjMzU1OmTFGtWrXk7u4uf39/de/eXadPn3Z4X8uWLXXXXXdpz549at68uTw9PVW1alVNmjRJmZmZkv73I/L09HRFR0fbf1wuSWPHjrX//feuv+f48eP2Zdu3b1fLli3l6+urEiVKKDg4WI8++qguX75s3ya7ywAOHjyozp07q2zZsvLw8FBISIiWLVvmsM31H5evXr1ao0ePVmBgoLy8vNS2bVsdPnw4d19kSU8++aQkafXq1fZlFy9e1DvvvKPevXtn+55x48apcePG8vHxkZeXlxo0aKBFixbJsiz7NpUrV9Z3332njz/+2P71u35m+vrsK1as0PDhw1WxYkW5u7vryJEjWS4DSEhIUFBQkJo2baq0tDT7/r///nuVLFlSTz/9dK6PFcBfF7EKwCgZGRnavn27wsLCFBQUlKv3PPPMM3rxxRfVrl07bdy4UePHj9fWrVvVtGlTJSQkOGwbHx+vbt266amnntLGjRvVoUMHjRo1SitXrpQkdezYUTExMZKkv//974qJibG/zq3jx4+rY8eOcnNz0+LFi7V161ZNmjRJJUuW1NWrV3N83+HDh9W0aVN99913mjlzptatW6c6deqoZ8+emjJlSpbtX3rpJZ04cUILFy7U/Pnz9eOPP6pTp07KyMjI1ZxeXl76+9//rsWLF9uXrV69Wk5OTnr88cdzPLb+/ftr7dq1WrdunR555BE999xzGj9+vH2b9evXq2rVqgoNDbV//f54ycaoUaN08uRJzZ07V5s2bZK/v3+Wz/Lz89OaNWu0Z88evfjii5Kky5cv67HHHlNwcLDmzp2bq+ME8BdnAYBB4uPjLUnWE088kavtY2NjLUnWwIEDHZbv3r3bkmS99NJL9mUtWrSwJFm7d+922LZOnTrW/fff77BMkjVo0CCHZZGRkVZ23zaXLFliSbKOHTtmWZZlvf3225Yka//+/TecXZIVGRlpf/3EE09Y7u7u1smTJx2269Chg+Xp6WlduHDBsizL2rFjhyXJevDBBx22W7t2rSXJiomJueHnXp93z5499n0dPHjQsizLuueee6yePXtalmVZdevWtVq0aJHjfjIyMqy0tDTr1VdftXx9fa3MzEz7upzee/3z7rvvvhzX7dixw2H55MmTLUnW+vXrrR49elglSpSwvvnmmxseI4CigzOrAP7SduzYIUlZbuRp1KiRateurW3btjksDwgIUKNGjRyW1atXTydOnMi3mUJCQuTm5qZ+/fpp2bJlOnr0aK7et337drVp0ybLGeWePXvq8uXLWc7w/v5SCOnacUjK07G0aNFC1apV0+LFi/Xtt99qz549OV4CcH3Gtm3bytvbW87OznJ1ddUrr7yi8+fP6+zZs7n+3EcffTTX244YMUIdO3bUk08+qWXLlmnWrFm6++67c/1+AH9txCoAo/j5+cnT01PHjh3L1fbnz5+XJFWoUCHLusDAQPv663x9fbNs5+7urpSUlJuYNnvVqlXTRx99JH9/fw0aNEjVqlVTtWrVNGPGjBu+7/z58zkex/X1v/fHY7l+fW9ejsVms6lXr15auXKl5s6dqxo1aqh58+bZbvvll1+qffv2kq49reHzzz/Xnj17NHr06Dx/bnbHeaMZe/bsqStXriggIIBrVYFihlgFYBRnZ2e1adNGX331VZYbpLJzPdji4uKyrDtz5oz8/PzybTYPDw9JUmpqqsPyP14XK0nNmzfXpk2bdPHiRe3atUtNmjTRkCFDtGbNmhz37+vrm+NxSMrXY/m9nj17KiEhQXPnzlWvXr1y3G7NmjVydXXV5s2b1aVLFzVt2lQNGza8qc/M7ka1nMTFxWnQoEEKCQnR+fPn9cILL9zUZwL4ayJWARhn1KhRsixLffv2zfaGpLS0NG3atEmS1Lp1a0my3yB13Z49exQbG6s2bdrk21zX72j/5ptvHJZfnyU7zs7Oaty4saKioiRJX3/9dY7btmnTRtu3b7fH6XXLly+Xp6dngT3WqWLFihoxYoQ6deqkHj165LidzWaTi4uLnJ2d7ctSUlK0YsWKLNvm19nqjIwMPfnkk7LZbNqyZYsmTpyoWbNmad26dbe8bwB/DTxnFYBxmjRpoujoaA0cOFBhYWF65plnVLduXaWlpWnfvn2aP3++7rrrLnXq1Ek1a9ZUv379NGvWLDk5OalDhw46fvy4xowZo6CgIA0dOjTf5nrwwQfl4+OjiIgIvfrqq3JxcdHSpUt16tQph+3mzp2r7du3q2PHjgoODtaVK1fsd9y3bds2x/1HRkZq8+bNatWqlV555RX5+Pho1apVeu+99zRlyhR5e3vn27H80aRJk/50m44dO+pf//qXunbtqn79+un8+fOaOnVqto8Xu/vuu7VmzRq99dZbqlq1qjw8PG7qOtPIyEh9+umn+uCDDxQQEKDhw4fr448/VkREhEJDQ1WlSpU87xPAXwuxCsBIffv2VaNGjTRt2jRNnjxZ8fHxcnV1VY0aNdS1a1c9++yz9m2jo6NVrVo1LVq0SFFRUfL29tYDDzygiRMnZnuN6s3y8vLS1q1bNWTIED311FMqU6aM+vTpow4dOqhPnz727UJCQvTBBx8oMjJS8fHxKlWqlO666y5t3LjRfs1ndmrWrKkvvvhCL730kgYNGqSUlBTVrl1bS5YsydNvgioorVu31uLFizV58mR16tRJFStWVN++feXv76+IiAiHbceNG6e4uDj17dtXv/32mypVquTwHNrc+PDDDzVx4kSNGTPG4Qz50qVLFRoaqscff1yfffaZ3Nzc8uPwABjKZlm/e5IzAAAAYBCuWQUAAICxiFUAAAAYi1gFAACAsYhVAAAAGItYBQAAgLGIVQAAABiLWAUAAICxiuQvBSgR+uyfbwQAfyG/7pld2CMAQL7yyGWFcmYVAAAAxiJWAQAAYCxiFQAAAMYiVgEAAGAsYhUAAADGIlYBAABgLGIVAAAAxiJWAQAAYCxiFQAAAMYiVgEAAGAsYhUAAADGIlYBAABgLGIVAAAAxiJWAQAAYCxiFQAAAMYiVgEAAGAsYhUAAADGIlYBAABgLGIVAAAAxiJWAQAAYCxiFQAAAMYiVgEAAGAsYhUAAADGIlYBAABgLGIVAAAAxiJWAQAAYCxiFQAAAMYiVgEAAGAsYhUAAADGIlYBAABgLGIVAAAAxiJWAQAAYCxiFQAAAMYiVgEAAGAsYhUAAADGIlYBAABgLGIVAAAAxiJWAQAAYCxiFQAAAMYiVgEAAGAsYhUAAADGIlYBAABgLGIVAAAAxiJWAQAAYCxiFQAAAMYiVgEAAGAsYhUAAADGIlYBAABgLGIVAAAAxiJWAQAAYCxiFQAAAMYiVgEAAGAsYhUAAADGIlYBAABgLGIVAAAAxiJWAQAAYCxiFQAAAMYiVgEAAGAsYhUAAADGIlYBAABgLGIVAAAAxiJWAQAAYCxiFQAAAMYiVgEAAGAsYhUAAADGIlYBAABgLGIVAAAAxiJWAQAAYCxiFQAAAMYiVgEAAGAsYhUAAADGIlYBAABgLGIVAAAAxiJWAQAAYCxiFQAAAMYiVgEAAGAsYhUAAADGIlYBAABgLGIVAAAAxiJWAQAAYCxiFQAAAMYiVgEAAGAsYhUAAADGIlYBAABgLGIVAAAAxiJWAQAAYCxiFQAAAMYiVgEAAGAsYhUAAADGIlYBAABgLGIVAAAAxiJWAQAAYCxiFQAAAMYiVgEAAGAsYhUAAADGIlYBAABgLGIVAAAAxiJWAQAAYCxiFQAAAMYiVgEAAGAsYhUAAADGIlYBAABgLGIVAAAAxiJWAQAAYCxiFQAAAMYiVgEAAGAsYhUAAADGIlYBAABgLGIVAAAAxnIp7AGAwlTK012RAx/Sw63rq1zZUjpw+LRemPK2vvr+pFxcnDR2YCfd36yuqtzhq6RLV7R99yGNmblRcecu2vcxa/QTat24piqU89allFTtOnBML894Vz8c/8W+TfVgf00YGq4m9avKzdVZ3x05o7FRm/XJ3h8L47ABFCPRUbM0d85sh2W+vn7a/snnkiTLsjR3zmy98++3lJSUpLvr1deol19R9ep3OrznwP59mjVjmr799hu5urioZq3aipq7QB4eHrftWFA8Easo1qJf6ao61QPV++Vlijt3UU8+2EjvzX1ODR79py6lpCqkdpAmLdiib374WWW9PPX6C4/q39P7q1m3KfZ97Is9pTVb9uhU3K/y8fbU6AEdtXnOINV6KFKZmZYkaf2sAfrxxFl16D9TKalperZrK62bOUB1O43VL+d/K6zDB1BMVKt+p+YvXGJ/7eTsbP/7kkULtGLZEr362iRVqlxZC+ZFa0CfXnr3va0qWbKUpGuhOrB/H/Xu01//GD1Grq6u+uHQITk58QNaFDybZVlWYQ+R30qEPlvYI+AvwMPdVec+m6rHhs7X1s++sy/fteYf2vLJQY2bsznLe8LqBOuzVSNVo8MYnYr/Ndv93nVnoPasfUl1Oo3VsdMJ8i1TUqd3TFbb3tP0+b6fJF07o3vu8zfUof9M7fzyh4I5QBQpv+6Z/ecbAdmIjpqlHds+0tp172ZZZ1mW2rZsrm5Pd1fvPv0kSVevXlXr+5pq8LAX9FiXJyRJTz3ZRX9r0lTPPj/kdo6OIs4jl6dMC/WfRKdPn9bo0aPVqlUr1a5dW3Xq1FGrVq00evRonTp1qjBHQzHg4uwkFxdnXbma5rD8SmqamoZWy/Y9XqVLKDMzUxd+S8l2vaeHm7o//DcdO52g0/+N2fMXkhV7NE5dH2okTw83OTs7qc+jzRSfkKR93/P/cwAF78TJE2rbspk6tG+tkS8M1en//jf259OnlZBwTk3ubWbf1s3NTWEN79GBffskSefPn9e33xyQj6+vund7Qq3ua6rePZ7S11/tLZRjQfFTaJcBfPbZZ+rQoYOCgoLUvn17tW/fXpZl6ezZs9qwYYNmzZqlLVu26N57773hflJTU5WamuqwzMrMkM3JOYd3ANdcupyqXQeOalTfDjp87Bf9cj5JXR5oqHvuqqQjJ89l2d7dzUXjn++st7bs1W/JVxzW9XusuV4bEq5Snu46dDReHZ+ZrbT0DPv6hwbM1trp/XXu86nKzLR0NvE3dR4UpYuXso9eAMgvd9erp9cmTFalypV1/vx5LZgXre7dntC6jZuVkHDte52vr6/De3x9/XTmzBlJ0s+nr4Xt3KjZGjZipGrWqq3N725Qv4ieeufdzapUqfJtPR4UP4UWq0OHDlWfPn00bdq0HNcPGTJEe/bsueF+Jk6cqHHjxjkscy5/j1wrNMq3WVF09X55ueaN7aajH7ym9PQM7T90Sm9t2auQ2kEO27m4OGnFpF5ystk0eOLaLPtZs2WPtu0+pAA/Lw3p3lYrJ/dW617/UurVdEnS9Jce17nE39S293SlpF5Vz/9rqnUzB6jZU68rPiHpthwrgOKpWfMW9r/fKale/RA99EA7bdywQfXq15ck2Ww2h/dYlqXrizIzMyVJf+/yuML/71FJUu3adbR7d4w2rHtHg4cOL/iDQLFWaJcBHDx4UAMGDMhxff/+/XXw4ME/3c+oUaN08eJFhz8u5cPyc1QUYcdOJ6h9nxnybTJMd3YYo+ZPT5Wri7OO/3zevo2Li5NWTY5QpYq+euiZ2VnOqkpS0qUr+unkOX3+9U/q+sJC1axSXp1bX/uPQMtGNfRg87vU/R9LFHPgqPYfOq0hE9cqJTVNT3VqfNuOFQAkydPTU3fWqKGTJ4/Lz6+cJCkhIcFhm8TE8/L19ZMk+ZW7tk3Vao6XR1WpWk3xcWduw8Qo7gotVitUqKAvvvgix/UxMTGqUKHCn+7H3d1dXl5eDn+4BAB5dfnKVcUnJKlM6RJq27S2Nu/8VtL/QrVacDl1HDBbiReTc7U/m2xyc732gwtPDzdJ/zs7cV1mppXlbAYAFLSrV6/q6NGf5OdXThXvuEN+fuW064vP7evTrl7VV3v3qH5oqCSpYsU7VM7fX8ePHXPYz4njx1UhsOJtnR3FU6FdBvDCCy9owIAB+uqrr9SuXTuVL19eNptN8fHx+vDDD7Vw4UJNnz69sMZDMdG2SW3ZbNIPx8+qWlA5TRgarh+Pn9XyjTFydnbSm6/3UWitID0yeK6cnWwq71takpR48bLS0jNUuaKv/n5/mLbFxCrh10sK9C+j4T3bKiU1Tf/57xMGdn9zTL8mXdbC8d01Yf4WpVxJU+9HmqpyRV+HpxAAQEF44/XJatGylQIqVFBiYqIWzI1W8qVLejj8/2Sz2dTt6e5atGCegitVVnClSlo0f548PDz0YMeHJF27RKBnrwhFR81SzZq1VLNWbW18d72OHzuqN6bNLOSjQ3FQaLE6cOBA+fr6atq0aZo3b54yMq7djOLs7KywsDAtX75cXbp0KazxUEx4l/LQq889rIrlyyjx4mW9u22/IqM2KT09U8EVfNSpZT1J0pdvjXJ4X/s+M/TpVz8q9Wq67g2tpme7tlRZL0+dPf+bPvv6iFr1fEPnfr0k6drTADo/O0djB3XSlnnPy9XFSbFH4/XY0Pn69oefb/sxAyhefvklXv8YMUy//npBZX3Kql69EK14c60C/3tWtFdEX6WmpmrC+HFKSrqou+vVV/SCxfZnrErSU917KjX1ql6fMlEXL15UzZq1NHfBYgUFBxfWYaEYMeI5q2lpafbrZfz8/OTq6npL++M5qwCKGp6zCqCoye1zVo34DVaurq65uj4VAAAAxQu/Jw0AAADGIlYBAABgLGIVAAAAxiJWAQAAYCxiFQAAAMYiVgEAAGAsYhUAAADGIlYBAABgLGIVAAAAxiJWAQAAYCxiFQAAAMYiVgEAAGAsYhUAAADGIlYBAABgLGIVAAAAxiJWAQAAYCxiFQAAAMYiVgEAAGAsYhUAAADGIlYBAABgLGIVAAAAxiJWAQAAYCxiFQAAAMYiVgEAAGAsYhUAAADGIlYBAABgLGIVAAAAxiJWAQAAYCxiFQAAAMYiVgEAAGAsYhUAAADGIlYBAABgLGIVAAAAxiJWAQAAYCxiFQAAAMYiVgEAAGAsYhUAAADGIlYBAABgLGIVAAAAxiJWAQAAYCxiFQAAAMYiVgEAAGAsYhUAAADGIlYBAABgLGIVAAAAxiJWAQAAYCxiFQAAAMYiVgEAAGAsYhUAAADGIlYBAABgLGIVAAAAxiJWAQAAYCxiFQAAAMYiVgEAAGAsYhUAAADGIlYBAABgLGIVAAAAxiJWAQAAYCxiFQAAAMYiVgEAAGAsYhUAAADGIlYBAABgLGIVAAAAxiJWAQAAYCxiFQAAAMYiVgEAAGAsYhUAAADGIlYBAABgLGIVAAAAxiJWAQAAYCxiFQAAAMYiVgEAAGAsYhUAAADGIlYBAABgLGIVAAAAxnLJzUYbN27M9Q4ffvjhmx4GAAAA+L1cxWp4eHiudmaz2ZSRkXEr8wAAAAB2uYrVzMzMgp4DAAAAyOKWrlm9cuVKfs0BAAAAZJHnWM3IyND48eNVsWJFlSpVSkePHpUkjRkzRosWLcr3AQEAAFB85TlWX3vtNS1dulRTpkyRm5ubffndd9+thQsX5utwAAAAKN7yHKvLly/X/Pnz1a1bNzk7O9uX16tXT4cOHcrX4QAAAFC85TlWf/75Z1WvXj3L8szMTKWlpeXLUAAAAIB0E7Fat25dffrpp1mW//vf/1ZoaGi+DAUAAABIuXx01e9FRkbq6aef1s8//6zMzEytW7dOhw8f1vLly7V58+aCmBEAAADFVJ7PrHbq1ElvvfWW3n//fdlsNr3yyiuKjY3Vpk2b1K5du4KYEQAAAMWUzbIsq7CHyG8lQp8t7BEAIF/9umd2YY8AAPnKI5c/38/zZQDX7d27V7GxsbLZbKpdu7bCwsJudlcAAABAtvIcq6dPn9aTTz6pzz//XGXKlJEkXbhwQU2bNtXq1asVFBSU3zMCAACgmMrzNau9e/dWWlqaYmNjlZiYqMTERMXGxsqyLEVERBTEjAAAACim8nzNaokSJfTFF19keUzV119/rXvvvVcpKSn5OuDN4JpVAEUN16wCKGpye81qns+sBgcHZ/vw//T0dFWsWDGvuwMAAABylOdYnTJlip577jnt3btX10/K7t27V4MHD9bUqVPzfUAAAAAUX7m6DKBs2bKy2Wz218nJyUpPT5eLy7Xzt9f/XrJkSSUmJhbctLnEZQAAihouAwBQ1OTro6umT59+C6MAAAAANydXsdqjR4+CngMAAADI4qZ/KYAkpaSkZLnZysvL65YGAgAAAK7L8w1WycnJevbZZ+Xv769SpUqpbNmyDn8AAACA/JLnWB05cqS2b9+uOXPmyN3dXQsXLtS4ceMUGBio5cuXF8SMAAAAKKbyfBnApk2btHz5crVs2VK9e/dW8+bNVb16dVWqVEmrVq1St27dCmJOAAAAFEN5PrOamJioKlWqSLp2fer1R1U1a9ZMn3zySf5OBwAAgGItz7FatWpVHT9+XJJUp04drV27VtK1M65lypTJz9kAAABQzOU5Vnv16qUDBw5IkkaNGmW/dnXo0KEaMWJEvg8IAACA4itXv8HqRk6ePKm9e/eqWrVqql+/fn7NdUv4DVYAihp+gxWAoia3v8Eqz2dW/yg4OFiPPPKIfHx81Lt371vdHQAAAGB3y7F6XWJiopYtW5ZfuwMAAADyL1YBAACA/EasAgAAwFjEKgAAAIyV699g9cgjj9xw/YULF251lnxz8pPphT0CAOSr1LTMwh4BAPKVh0vuzpnmOla9vb3/dH337t1zuzsAAADgT93yc1ZNdO639MIeAQDylVsuz0AAwF+Fd4ncfV/jux8AAACMRawCAADAWMQqAAAAjEWsAgAAwFjEKgAAAIx1U7G6YsUK3XvvvQoMDNSJEyckSdOnT9e7776br8MBAACgeMtzrEZHR2vYsGF68MEHdeHCBWVkZEiSypQpo+nTp+f3fAAAACjG8hyrs2bN0oIFCzR69Gg5Ozvblzds2FDffvttvg4HAACA4i3PsXrs2DGFhoZmWe7u7q7k5OR8GQoAAACQbiJWq1Spov3792dZvmXLFtWpUyc/ZgIAAAAkSS55fcOIESM0aNAgXblyRZZl6csvv9Tq1as1ceJELVy4sCBmBAAAQDFlsyzLyuubFixYoH/+8586deqUJKlixYoaO3asIiIi8n3Am3Hut/TCHgEA8pWbC08aBFC0eJfI3fe1m4rV6xISEpSZmSl/f/+b3UWBIFYBFDXEKoCiJrexmufLAH7Pz8/vVt4OAAAA3FCeY7VKlSqy2Ww5rj969OgtDQQAAABcl+dYHTJkiMPrtLQ07du3T1u3btWIESPyay4AAAAg77E6ePDgbJdHRUVp7969tzwQAAAAcN0t3WD1e0ePHlVISIiSkpLyY3e3hBusABQ13GAFoKjJ7Q1W+fbd7+2335aPj09+7Q4AAADI+2UAoaGhDjdYWZal+Ph4nTt3TnPmzMnX4QAAAFC85TlWw8PDHV47OTmpXLlyatmypWrVqpVfcwEAAAB5i9X09HRVrlxZ999/vwICAgpqJgAAAEDSTdxg5enpqdjYWFWqVKmgZrpl3GAFoKjhBisARU2B3WDVuHFj7du3L88DAQAAAHmV52tWBw4cqOHDh+v06dMKCwtTyZIlHdbXq1cv34YDAABA8ZbrywB69+6t6dOnq0yZMll3YrPJsizZbDZlZGTk94x5xmUAAIoaLgMAUNTk9jKAXMeqs7Oz4uLilJKScsPtTLiWlVgFUNQQqwCKmtzGaq4vA7jetCbEKAAAAIqHPP1T/fe/DAAAAAAoaHm6wapGjRp/GqyJiYm3NBAAAABwXZ5iddy4cfL29i6oWQAAAAAHub7BysnJSfHx8fL39y/omW4ZN1gBKGq4wQpAUZPvvxSA61UBAABwu+U6VvP4W1kBAACAW5bra1YzMzMLcg4AAAAgCy6CAgAAgLGIVQAAABiLWAUAAICxiFUAAAAYi1gFAACAsYhVAAAAGItYBQAAgLGIVQAAABiLWAUAAICxiFUAAAAYi1gFAACAsYhVAAAAGItYBQAAgLGIVQAAABiLWAUAAICxiFUAAAAYi1gFAACAsYhVAAAAGItYBQAAgLGIVQAAABiLWAUAAICxiFUAAAAYi1gFAACAsYhVAAAAGItYBQAAgLGIVQAAABiLWAUAAICxiFUAAAAYi1gFAACAsYhVAAAAGItYBQAAgLGIVQAAABiLWAUAAICxiFUAAAAYi1gFAACAsYhVAAAAGItYBQAAgLGIVQAAABiLWAUAAICxiFUAAAAYi1gFAACAsYhVAAAAGItYBQAAgLGIVQAAABiLWAUAAICxiFUAAAAYi1gFAACAsYhVAAAAGItYBQAAgLGIVQAAABiLWAUAAICxiFUAAAAYi1gFAACAsYhVAAAAGItYBQAAgLGIVQAAABiLWAUAAICxiFUAAAAYi1gFAACAsYhVAAAAGItYBQAAgLGIVQAAABiLWAUAAICxiFUAAAAYi1gFAACAsYhVAAAAGItYBQAAgLGIVQAAABiLWAUAAICxiFUAAAAYi1gFAACAsYhVAAAAGMulsAcACtP+r/fqzRWLdTj2e51POKcJU2fqvpZt7Os/3v6h3l23Vodjv9fFixe0ZNXburNmbYd9XL16VVHTX9dH/3lfqampCrunsYb/Y4z8ywc4bPfFZx9ryYJo/XTkB5XwKKH6DRpqwuszbstxAii+0tPTtWDubG19f7MSzyfI16+cHno4XL37PiMnp2vnrHZs+0Dr3l6rQ7Hf6eKFC1q5Zp1q1HL8XpeQcE6zpr2u3btidDk5WZUqV1bPiP5q0+7+wjgsFCOcWUWxlpKSoup31tSwkaNzXH93/VANeG5ojvuY+cYkfbJzm8ZOmKo5C1coJeWyRg4dqIyMDPs2O7d9oPGv/EMdO/2flr65TtGLVqrd/Q/m+/EAwB8tX7JQ695+SyP+8bLeWveenhvyglYuW6y1q1fat0lJSVH9kFANen5YjvsZO/pFnTh+XG9Mj9Lqt99VyzbtNPrFYTp86PvbcRgoxjizimKtyb3N1eTe5jmuf6Djw5KkuDM/Z7v+0qXftPnddzTm1Um6p3ETSdIr4yfrkY5ttPfLGDVu0kzp6ema8cYkDXr+BT0U/qj9vcGVq+TjkQBA9r79Zr/ua9laze5rKUkKrFhRH2x9T7HfH7Rv8+BDnSVJZ37O/nvdtf0c0IujX1Hdu+tJkiL6PqPVK5fpUOz3qlmrTsEdAIo9zqwCt+Bw7HdKT0/XPX9ral/mV85fVapV18Fv9kuSfjj0vc6d/UU2Jyf16vqoOt/fQsOf76+jPx0ppKkBFCchoWHau3uXTpw4Jkn64fAhHdj3tZo2a5Gn/dQPbaAP/7NFFy9eUGZmpj7Y+p7SrqYprGGjghgbsDP6zOqpU6cUGRmpxYsX57hNamqqUlNTHZdddZa7u3tBjwfo/PkEubq6ysvL22G5j4+fzickSJLO/HxakrR4fpSeGzpSAYEVtWblUj3Xr4dWr3tPXt5lbvfYAIqR7r366NKl39QlvKOcnJ2VmZGhZ54dovs7dMzTfiZM/pdeenGY2rVoImcXF3l4eGjKv2bqjqDgApocuMboM6uJiYlatmzZDbeZOHGivL29Hf7MeGPybZoQyJ5lWbLZbJKkTCtTktS9dz+1bNNetWrX1UuRr8lms2n7Rx8U5pgAioEP//O+try3SeMnvq4Vq99R5PiJWrl8sTZv3JCn/URHzdBvSUmaPW+xlq36t7o+1VOjRgzVkR9/KJjBgf8q1DOrGzduvOH6o0eP/uk+Ro0apWHDHC8IT7rqfEtzAbnl6+untLQ0JSVddDi7+uuv53V3/RBJkp9fOUlS5arV7Ovd3NxUoeId+iU+7rbOC6D4mTltqnr06qP2D1w7k1r9zhqKizujZYvn66GHw3O1j9OnTurfa1Zp9dsbVa36nZKkGjVraf++vfr3W29q1MtjC2h6oJBjNTw8XDabTZZl5bjN9bNTOXF3d8/yI//U39LzZT7gz9SsXVcuLi7asztGbdo9IOna412O/XREA58ffm2bWnXl5uamU8ePq35ImCQpPT1N8XFnFFChQqHNDqB4uHIlRTYnxx+kOjs5KzMzMw/7uCJJ9kddXefk5CwrD/sBbkahxmqFChUUFRWl8PDwbNfv379fYWFht3coFCuXLyfr51Mn7a/jfj6tHw/HqrS3twICApV08YJ+iY9TwrlzkqSTJ45Lknx8/eTrV06lSpXWQ50fVdT01+XtXUZeXt6KmvG6qla/Uw0bXXs6QMlSpdT50S5aND9K/gEBCggI1JsrlkiSWrXl+YQAClbz+1pp6cJ5CgiooKrV7tThw9/rzZVL1anzI/ZtLl68oF/i4nTu3FlJst+M5ePnJz+/cqpcuYqCgoI18Z+RGjx0pLzLlNHHO7bpy11f6F8zowvluFB82KwbndYsYA8//LBCQkL06quvZrv+wIEDCg0NzdO//iTpHGdWkUtf7/1Szw/olWV5h4c6a/TYCXp/03pNGPdylvW9+g5URP9Bkq7d5DdnxlR9+J/3lHolVWGNGmv4i2NUPuB/Z03T09M0d/Z0/ef9TUpNvaI6devp+eH/UNVq1Qvu4FCkuLkYfYsBDJacnKx5UTO0c8dH+jUxUX7l/NX+gQfVp/9Aubq6SZI2v7ter0a+lOW9ffoPUr9nnpV07R/rUTP/pQP7vtbly5d1R3Cwnurey/7YKyCvvEvk7vtaocbqp59+quTkZD3wwAPZrk9OTtbevXvVokXeHq9BrAIoaohVAEXNXyJWCwqxCqCoIVYBFDW5jVW++wEAAMBYxCoAAACMRawCAADAWMQqAAAAjEWsAgAAwFjEKgAAAIxFrAIAAMBYxCoAAACMRawCAADAWMQqAAAAjEWsAgAAwFjEKgAAAIxFrAIAAMBYxCoAAACMRawCAADAWMQqAAAAjEWsAgAAwFjEKgAAAIxFrAIAAMBYxCoAAACMRawCAADAWMQqAAAAjEWsAgAAwFjEKgAAAIxFrAIAAMBYxCoAAACMRawCAADAWMQqAAAAjEWsAgAAwFjEKgAAAIxFrAIAAMBYxCoAAACMRawCAADAWMQqAAAAjEWsAgAAwFjEKgAAAIxFrAIAAMBYxCoAAACMRawCAADAWMQqAAAAjEWsAgAAwFjEKgAAAIxFrAIAAMBYxCoAAACMRawCAADAWMQqAAAAjEWsAgAAwFjEKgAAAIxFrAIAAMBYxCoAAACMRawCAADAWMQqAAAAjEWsAgAAwFjEKgAAAIxFrAIAAMBYxCoAAACMRawCAADAWMQqAAAAjEWsAgAAwFjEKgAAAIxFrAIAAMBYxCoAAACMRawCAADAWMQqAAAAjEWsAgAAwFjEKgAAAIxFrAIAAMBYxCoAAACMRawCAADAWMQqAAAAjEWsAgAAwFjEKgAAAIxFrAIAAMBYxCoAAACMRawCAADAWMQqAAAAjEWsAgAAwFjEKgAAAIxFrAIAAMBYxCoAAACMRawCAADAWMQqAAAAjEWsAgAAwFjEKgAAAIxFrAIAAMBYxCoAAACMRawCAADAWMQqAAAAjEWsAgAAwFjEKgAAAIxFrAIAAMBYxCoAAACMRawCAADAWMQqAAAAjEWsAgAAwFjEKgAAAIxFrAIAAMBYxCoAAACMRawCAADAWMQqAAAAjEWsAgAAwFjEKgAAAIxFrAIAAMBYxCoAAACMRawCAADAWMQqAAAAjEWsAgAAwFjEKgAAAIxFrAIAAMBYNsuyrMIeAvgrSk1N1cSJEzVq1Ci5u7sX9jgAcMv4vgYTEavATUpKSpK3t7cuXrwoLy+vwh4HAG4Z39dgIi4DAAAAgLGIVQAAABiLWAUAAICxiFXgJrm7uysyMpKbEAAUGXxfg4m4wQoAAADG4swqAAAAjEWsAgAAwFjEKgAAAIxFrAIAAMBYxCpwk+bMmaMqVarIw8NDYWFh+vTTTwt7JAC4KZ988ok6deqkwMBA2Ww2bdiwobBHAuyIVeAmvPXWWxoyZIhGjx6tffv2qXnz5urQoYNOnjxZ2KMBQJ4lJyerfv36mj17dmGPAmTBo6uAm9C4cWM1aNBA0dHR9mW1a9dWeHi4Jk6cWIiTAcCtsdlsWr9+vcLDwwt7FEASZ1aBPLt69aq++uortW/f3mF5+/bt9cUXXxTSVAAAFE3EKpBHCQkJysjIUPny5R2Wly9fXvHx8YU0FQAARROxCtwkm83m8NqyrCzLAADArSFWgTzy8/OTs7NzlrOoZ8+ezXK2FQAA3BpiFcgjNzc3hYWF6cMPP3RY/uGHH6pp06aFNBUAAEWTS2EPAPwVDRs2TE8//bQaNmyoJk2aaP78+Tp58qQGDBhQ2KMBQJ5dunRJR44csb8+duyY9u/fLx8fHwUHBxfiZACPrgJu2pw5czRlyhTFxcXprrvu0rRp03TfffcV9lgAkGc7d+5Uq1atsizv0aOHli5devsHAn6HWAUAAICxuGYVAAAAxiJWAQAAYCxiFQAAAMYiVgEAAGAsYhUAAADGIlYBAABgLGIVAAAAxiJWAQAAYCxiFQBu0dixYxUSEmJ/3bNnT4WHh9/2OY4fPy6bzab9+/cX2Gf88Vhvxu2YE0DRQawCKJJ69uwpm80mm80mV1dXVa1aVS+88IKSk5ML/LNnzJiR619RebvDrWXLlhoyZMht+SwAyA8uhT0AABSUBx54QEuWLFFaWpo+/fRT9enTR8nJyYqOjs6ybVpamlxdXfPlc729vfNlPwAAzqwCKMLc3d0VEBCgoKAgde3aVd26ddOGDRsk/e/H2YsXL1bVqlXl7u4uy7J08eJF9evXT/7+/vLy8lLr1q114MABh/1OmjRJ5cuXV+nSpRUREaErV644rP/jZQCZmZmaPHmyqlevLnd3dwUHB+u1116TJFWpUkWSFBoaKpvNppYtW9rft2TJEtWuXVseHh6qVauW5syZ4/A5X375pUJDQ+Xh4aGGDRtq3759t/w1e/HFF1WjRg15enqqatWqGjNmjNLS0rJsN2/ePAUFBcnT01OPPfaYLly44LD+z2YHgNzizCqAYqNEiRIO4XXkyBGtXbtW77zzjpydnSVJHTt2lI+Pj95//315e3tr3rx5atOmjX744Qf5+Pho7dq1ioyMVFRUlJo3b64VK1Zo5syZqlq1ao6fO2rUKC1YsEDTpk1Ts2bNFBcXp0OHDkm6FpyNGjXSRx99pLp168rNzU2StGDBAkVGRmr27NkKDQ3Vvn371LdvX5UsWVI9evRQcnKyHnroIbVu3VorV67UsWPHNHjw4Fv+GpUuXVpLly5VYGCgvv32W/Xt21elS5fWyJEjs3zdNm3apKSkJEVERGjQoEFatWpVrmYHgDyxAKAI6tGjh9W5c2f76927d1u+vr5Wly5dLMuyrMjISMvV1dU6e/asfZtt27ZZXl5e1pUrVxz2Va1aNWvevHmWZVlWkyZNrAEDBjisb9y4sVW/fv1sPzspKclyd3e3FixYkO2cx44dsyRZ+/btc1geFBRkvfnmmw7Lxo8fbzVp0sSyLMuaN2+e5ePjYyUnJ9vXR0dHZ7uv32vRooU1ePDgHNf/0ZQpU6ywsDD768jISMvZ2dk6deqUfdmWLVssJycnKy4uLlez53TMAJAdzqwCKLI2b96sUqVKKT09XWlpaercubNmzZplX1+pUiWVK1fO/vqrr77SpUuX5Ovr67CflJQU/fTTT5Kk2NhYDRgwwGF9kyZNtGPHjmxniI2NVWpqqtq0aZPruc+dO6dTp04pIiJCffv2tS9PT0+3Xw8bGxur+vXry9PT02GOW/X2229r+vTpOnLkiC5duqT09HR5eXk5bBMcHKw77rjD4XMzMzN1+PBhOTs7/+nsAJAXxCqAIqtVq1aKjo6Wq6urAgMDs9xAVbJkSYfXmZmZqlChgnbu3JllX2XKlLmpGUqUKJHn92RmZkq69uP0xo0bO6y7frmCZVk3Nc+N7Nq1S0888YTGjRun+++/X97e3lqzZo3eeOONG77PZrPZ/zc3swNAXhCrAIqskiVLqnr16rnevkGDBoqPj5eLi4sqV66c7Ta1a9fWrl271L17d/uyXbt25bjPO++8UyVKlNC2bdvUp0+fLOuvX6OakZFhX1a+fHlVrFhRR48eVbdu3bLdb506dbRixQqlpKTYg/hGc+TG559/rkqVKmn06NH2ZSdOnMiy3cmTJ3XmzBkFBgZKkmJiYuTk5KQaNWrkanYAyAtiFQD+q23btmrSpInCw8M1efJk1axZU2fOnNH777+v8PBwNWzYUIMHD1aPHj3UsGFDNWvWTKtWrdJ3332X4w1WHh4eevHFFzVy5Ei5ubnp3nvv1blz5/Tdd98pIiJC/v7+KlGihLZu3ao77rhDHh4e8vb21tixY/X888/Ly8tLHTp0UGpqqvbu3atff/1Vw4YNU9euXTV69GhFRETo5Zdf1vHjxzV16tRcHee5c+eyPNc1ICBA1atX18mTJ7VmzRrdc889eu+997R+/fpsj6lHjx6aOnWqkpKS9Pzzz6tLly4KCAiQpD+dHQDypLAvmgWAgvDHG6z+KDIy0uGmqOuSkpKs5557zgoMDLRcXV2toKAgq1u3btbJkyft27z22muWn5+fVapUKatHjx7WyJEjc7zByrIsKyMjw/rnP/9pVapUyXJ1dbWCg4OtCRMm2NcvWLDACgoKspycnKwWLVrYl69atcoKCQmx3NzcrLJly1r33XeftW7dOvv6mJgYq379+pabm5sVEhJivfPOO7m6wUpSlj+RkZGWZVnWiBEjLF9fX6tUqVLW448/bk2bNs3y9vbO8nWbM2eOFRgYaHl4eFiPPPKIlZiY6PA5N5qdG6wA5IXNsgrgwicAAAAgH/BLAQAAAGAsYhUAAADGIlYBAABgLGIVAAAAxiJWAQAAYCxiFQAAAMYiVgEAAGAsYhUAAADGIlYBAABgLGIVAAAAxiJWAQAAYKz/B3d3HxJ4Bsl/AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_dataloader:\n",
    "        outputs = model(inputs)\n",
    "        # Stored true labels and predictions\n",
    "        y_true.extend(labels.numpy())\n",
    "        y_pred.extend(outputs.numpy())\n",
    "\n",
    "# Converted y_true and y_pred to numpy arrays\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "# Computed confusion matrix\n",
    "cm = confusion_matrix(np.round(y_true), np.round(y_pred))\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e35cbdc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.015088625\n",
      "Mean Absolute Error: 0.08259779\n",
      "Root Mean Squared Error: 0.12283576\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Computed Mean Squared Error\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "\n",
    "# Computed Mean Absolute Error\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "# Computed Root Mean Squared Error\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# Computed R2-Score\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"Root Mean Squared Error:\", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "877c41ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alarm triggered for 27 consecutive faults on 2019-07-15.\n",
      "Alarm triggered for 22 consecutive faults on 2019-08-09.\n",
      "Alarm triggered for 22 consecutive faults on 2019-08-20.\n",
      "Alarm triggered for 22 consecutive faults on 2019-07-21.\n",
      "Alarm triggered for 21 consecutive faults on 2019-07-30.\n",
      "Alarm triggered for 20 consecutive faults on 2019-08-14.\n",
      "Alarm triggered for 20 consecutive faults on 2019-07-17.\n",
      "Alarm triggered for 20 consecutive faults on 2019-06-27.\n",
      "Alarm triggered for 20 consecutive faults on 2019-07-10.\n",
      "Alarm triggered for 20 consecutive faults on 2019-06-21.\n",
      "Alarm triggered for 20 consecutive faults on 2019-07-11.\n",
      "Alarm triggered for 20 consecutive faults on 2019-08-24.\n",
      "Alarm triggered for 20 consecutive faults on 2019-08-17.\n",
      "Alarm triggered for 19 consecutive faults on 2019-07-18.\n",
      "Alarm triggered for 19 consecutive faults on 2019-06-10.\n",
      "Alarm triggered for 19 consecutive faults on 2019-07-08.\n",
      "Alarm triggered for 18 consecutive faults on 2019-06-22.\n",
      "Alarm triggered for 18 consecutive faults on 2019-07-06.\n",
      "Alarm triggered for 18 consecutive faults on 2019-08-23.\n",
      "Alarm triggered for 18 consecutive faults on 2019-08-01.\n",
      "Alarm triggered for 18 consecutive faults on 2019-07-29.\n",
      "Alarm triggered for 17 consecutive faults on 2019-06-20.\n",
      "Alarm triggered for 17 consecutive faults on 2019-07-20.\n",
      "Alarm triggered for 17 consecutive faults on 2019-07-25.\n",
      "Alarm triggered for 17 consecutive faults on 2019-08-15.\n",
      "Alarm triggered for 17 consecutive faults on 2019-06-11.\n",
      "Alarm triggered for 17 consecutive faults on 2019-08-21.\n",
      "Alarm triggered for 16 consecutive faults on 2019-07-31.\n",
      "Alarm triggered for 16 consecutive faults on 2019-07-09.\n",
      "Alarm triggered for 16 consecutive faults on 2019-08-13.\n",
      "Alarm triggered for 16 consecutive faults on 2019-07-05.\n",
      "Alarm triggered for 16 consecutive faults on 2019-07-07.\n",
      "Alarm triggered for 16 consecutive faults on 2019-08-27.\n",
      "Alarm triggered for 16 consecutive faults on 2019-06-09.\n",
      "Alarm triggered for 16 consecutive faults on 2019-06-14.\n",
      "Alarm triggered for 15 consecutive faults on 2019-07-19.\n",
      "Alarm triggered for 15 consecutive faults on 2019-07-01.\n",
      "Alarm triggered for 15 consecutive faults on 2019-06-29.\n",
      "Alarm triggered for 15 consecutive faults on 2019-06-25.\n",
      "Alarm triggered for 15 consecutive faults on 2019-08-16.\n",
      "Alarm triggered for 15 consecutive faults on 2019-08-11.\n",
      "Alarm triggered for 15 consecutive faults on 2019-06-19.\n",
      "Alarm triggered for 15 consecutive faults on 2019-06-07.\n",
      "Alarm triggered for 15 consecutive faults on 2019-06-18.\n",
      "Alarm triggered for 15 consecutive faults on 2019-08-25.\n",
      "Alarm triggered for 15 consecutive faults on 2019-08-08.\n",
      "Alarm triggered for 14 consecutive faults on 2019-08-05.\n",
      "Alarm triggered for 14 consecutive faults on 2019-06-08.\n",
      "Alarm triggered for 14 consecutive faults on 2019-07-24.\n",
      "Alarm triggered for 14 consecutive faults on 2019-06-23.\n",
      "Alarm triggered for 13 consecutive faults on 2019-08-18.\n",
      "Alarm triggered for 13 consecutive faults on 2019-06-12.\n",
      "Alarm triggered for 13 consecutive faults on 2019-08-22.\n",
      "Alarm triggered for 13 consecutive faults on 2019-08-04.\n",
      "Alarm triggered for 13 consecutive faults on 2019-06-04.\n",
      "Alarm triggered for 13 consecutive faults on 2019-06-24.\n",
      "Alarm triggered for 13 consecutive faults on 2019-07-12.\n",
      "Alarm triggered for 13 consecutive faults on 2019-06-13.\n",
      "Alarm triggered for 13 consecutive faults on 2019-07-27.\n",
      "Alarm triggered for 13 consecutive faults on 2019-07-04.\n",
      "Alarm triggered for 12 consecutive faults on 2019-07-28.\n",
      "Alarm triggered for 12 consecutive faults on 2019-08-10.\n",
      "Alarm triggered for 12 consecutive faults on 2019-07-26.\n",
      "Alarm triggered for 12 consecutive faults on 2019-07-14.\n",
      "Alarm triggered for 12 consecutive faults on 2019-07-22.\n",
      "Alarm triggered for 12 consecutive faults on 2019-06-15.\n",
      "Alarm triggered for 11 consecutive faults on 2019-08-19.\n",
      "Alarm triggered for 11 consecutive faults on 2019-08-07.\n",
      "Alarm triggered for 11 consecutive faults on 2019-08-06.\n",
      "Alarm triggered for 11 consecutive faults on 2019-08-26.\n",
      "Alarm triggered for 11 consecutive faults on 2019-06-06.\n",
      "Alarm triggered for 11 consecutive faults on 2019-06-01.\n",
      "Alarm triggered for 10 consecutive faults on 2019-07-16.\n",
      "Alarm triggered for 10 consecutive faults on 2019-06-30.\n",
      "Alarm triggered for 10 consecutive faults on 2019-06-17.\n",
      "Alarm triggered for 10 consecutive faults on 2019-07-03.\n",
      "Alarm triggered for 9 consecutive faults on 2019-08-12.\n",
      "Alarm triggered for 9 consecutive faults on 2019-06-05.\n",
      "Alarm triggered for 9 consecutive faults on 2019-06-16.\n",
      "Alarm triggered for 9 consecutive faults on 2019-06-28.\n",
      "Alarm triggered for 9 consecutive faults on 2019-06-26.\n",
      "Alarm triggered for 9 consecutive faults on 2019-07-02.\n",
      "Alarm triggered for 9 consecutive faults on 2019-06-02.\n",
      "Alarm triggered for 8 consecutive faults on 2019-08-02.\n",
      "Alarm triggered for 8 consecutive faults on 2019-06-03.\n",
      "Alarm triggered for 8 consecutive faults on 2019-08-29.\n",
      "Alarm triggered for 8 consecutive faults on 2019-08-31.\n",
      "Alarm triggered for 7 consecutive faults on 2019-08-03.\n",
      "Alarm triggered for 7 consecutive faults on 2019-07-13.\n",
      "Alarm triggered for 7 consecutive faults on 2019-08-28.\n",
      "Alarm triggered for 5 consecutive faults on 2019-08-30.\n"
     ]
    }
   ],
   "source": [
    "# Function to check for four consecutive faults in a day and trigger alarms where Threshold = 2 * sigma\n",
    "def check_consecutive_faults(fault_indices, df, window=3):\n",
    "    fault_timestamps = df.index[fault_indices]\n",
    "\n",
    "    grouped_by_day = fault_timestamps.to_series().dt.date\n",
    "\n",
    "    # Checked if there are three or more consecutive faults within the same day\n",
    "    daily_fault_count = grouped_by_day.value_counts()\n",
    "    for date, count in daily_fault_count.items():\n",
    "        if count >= window:\n",
    "            print(f\"Alarm triggered for {count} consecutive faults on {date}.\")\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "\n",
    "with torch.no_grad():\n",
    "    predicted_ac_power = model(X_test_tensor)  # Model predictions\n",
    "    actual_ac_power = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "    # Absolute residuals between predicted and actual AC power\n",
    "    residuals = torch.abs(predicted_ac_power - actual_ac_power)\n",
    "\n",
    "# Calculated the standard deviation for setting the fault threshold\n",
    "sigma = torch.std(residuals).item()\n",
    "threshold = 2 * sigma  # Set the threshold as 2 * sigma\n",
    "\n",
    "anomalies = (residuals > threshold).int()\n",
    "\n",
    "fault_indices = torch.nonzero(anomalies).squeeze().tolist()\n",
    "\n",
    "check_consecutive_faults(fault_indices, df, window=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa207793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alarm triggered for 15 consecutive faults on 2019-07-15.\n",
      "Alarm triggered for 13 consecutive faults on 2019-08-09.\n",
      "Alarm triggered for 12 consecutive faults on 2019-08-14.\n",
      "Alarm triggered for 11 consecutive faults on 2019-08-24.\n",
      "Alarm triggered for 11 consecutive faults on 2019-07-17.\n",
      "Alarm triggered for 10 consecutive faults on 2019-07-21.\n",
      "Alarm triggered for 10 consecutive faults on 2019-08-20.\n",
      "Alarm triggered for 10 consecutive faults on 2019-07-11.\n",
      "Alarm triggered for 10 consecutive faults on 2019-08-21.\n",
      "Alarm triggered for 10 consecutive faults on 2019-08-11.\n",
      "Alarm triggered for 10 consecutive faults on 2019-06-09.\n",
      "Alarm triggered for 10 consecutive faults on 2019-08-15.\n",
      "Alarm triggered for 9 consecutive faults on 2019-06-27.\n",
      "Alarm triggered for 9 consecutive faults on 2019-07-09.\n",
      "Alarm triggered for 9 consecutive faults on 2019-07-05.\n",
      "Alarm triggered for 9 consecutive faults on 2019-08-23.\n",
      "Alarm triggered for 9 consecutive faults on 2019-06-11.\n",
      "Alarm triggered for 9 consecutive faults on 2019-07-18.\n",
      "Alarm triggered for 9 consecutive faults on 2019-07-20.\n",
      "Alarm triggered for 8 consecutive faults on 2019-07-19.\n",
      "Alarm triggered for 8 consecutive faults on 2019-07-30.\n",
      "Alarm triggered for 8 consecutive faults on 2019-07-08.\n",
      "Alarm triggered for 8 consecutive faults on 2019-07-06.\n",
      "Alarm triggered for 8 consecutive faults on 2019-07-29.\n",
      "Alarm triggered for 8 consecutive faults on 2019-07-01.\n",
      "Alarm triggered for 8 consecutive faults on 2019-07-12.\n",
      "Alarm triggered for 8 consecutive faults on 2019-06-07.\n",
      "Alarm triggered for 7 consecutive faults on 2019-08-01.\n",
      "Alarm triggered for 7 consecutive faults on 2019-08-05.\n",
      "Alarm triggered for 7 consecutive faults on 2019-08-16.\n",
      "Alarm triggered for 7 consecutive faults on 2019-07-25.\n",
      "Alarm triggered for 7 consecutive faults on 2019-07-07.\n",
      "Alarm triggered for 7 consecutive faults on 2019-07-24.\n",
      "Alarm triggered for 7 consecutive faults on 2019-06-14.\n",
      "Alarm triggered for 7 consecutive faults on 2019-06-29.\n",
      "Alarm triggered for 7 consecutive faults on 2019-06-28.\n",
      "Alarm triggered for 7 consecutive faults on 2019-06-08.\n",
      "Alarm triggered for 7 consecutive faults on 2019-06-16.\n",
      "Alarm triggered for 7 consecutive faults on 2019-06-20.\n",
      "Alarm triggered for 6 consecutive faults on 2019-07-04.\n",
      "Alarm triggered for 6 consecutive faults on 2019-07-10.\n",
      "Alarm triggered for 6 consecutive faults on 2019-08-06.\n",
      "Alarm triggered for 6 consecutive faults on 2019-06-13.\n",
      "Alarm triggered for 6 consecutive faults on 2019-07-31.\n",
      "Alarm triggered for 6 consecutive faults on 2019-06-06.\n",
      "Alarm triggered for 6 consecutive faults on 2019-08-17.\n",
      "Alarm triggered for 6 consecutive faults on 2019-08-19.\n",
      "Alarm triggered for 6 consecutive faults on 2019-06-19.\n",
      "Alarm triggered for 6 consecutive faults on 2019-06-21.\n",
      "Alarm triggered for 6 consecutive faults on 2019-06-22.\n",
      "Alarm triggered for 6 consecutive faults on 2019-06-24.\n",
      "Alarm triggered for 6 consecutive faults on 2019-06-25.\n",
      "Alarm triggered for 6 consecutive faults on 2019-06-30.\n",
      "Alarm triggered for 6 consecutive faults on 2019-08-27.\n",
      "Alarm triggered for 6 consecutive faults on 2019-08-26.\n",
      "Alarm triggered for 6 consecutive faults on 2019-08-10.\n",
      "Alarm triggered for 5 consecutive faults on 2019-08-18.\n",
      "Alarm triggered for 5 consecutive faults on 2019-08-28.\n",
      "Alarm triggered for 5 consecutive faults on 2019-06-23.\n",
      "Alarm triggered for 5 consecutive faults on 2019-07-13.\n",
      "Alarm triggered for 5 consecutive faults on 2019-08-13.\n",
      "Alarm triggered for 5 consecutive faults on 2019-06-01.\n",
      "Alarm triggered for 5 consecutive faults on 2019-08-08.\n",
      "Alarm triggered for 5 consecutive faults on 2019-08-07.\n",
      "Alarm triggered for 5 consecutive faults on 2019-06-10.\n",
      "Alarm triggered for 5 consecutive faults on 2019-06-12.\n",
      "Alarm triggered for 5 consecutive faults on 2019-07-28.\n",
      "Alarm triggered for 5 consecutive faults on 2019-07-27.\n",
      "Alarm triggered for 5 consecutive faults on 2019-07-26.\n",
      "Alarm triggered for 5 consecutive faults on 2019-06-18.\n",
      "Alarm triggered for 5 consecutive faults on 2019-06-26.\n",
      "Alarm triggered for 5 consecutive faults on 2019-07-16.\n",
      "Alarm triggered for 5 consecutive faults on 2019-07-14.\n",
      "Alarm triggered for 4 consecutive faults on 2019-08-12.\n",
      "Alarm triggered for 4 consecutive faults on 2019-06-15.\n",
      "Alarm triggered for 4 consecutive faults on 2019-07-03.\n",
      "Alarm triggered for 4 consecutive faults on 2019-08-25.\n",
      "Alarm triggered for 4 consecutive faults on 2019-08-22.\n",
      "Alarm triggered for 4 consecutive faults on 2019-07-22.\n"
     ]
    }
   ],
   "source": [
    "# Function to check for four consecutive faults in a day and trigger alarms where Threshold = 3 * sigma\n",
    "def check_consecutive_faults(fault_indices, df, window=3):\n",
    "    fault_timestamps = df.index[fault_indices]\n",
    "\n",
    "    grouped_by_day = fault_timestamps.to_series().dt.date\n",
    "\n",
    "    # Checked if there are three or more consecutive faults within the same day\n",
    "    daily_fault_count = grouped_by_day.value_counts()\n",
    "    for date, count in daily_fault_count.items():\n",
    "        if count >= window:\n",
    "            print(f\"Alarm triggered for {count} consecutive faults on {date}.\")\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "\n",
    "with torch.no_grad():\n",
    "    predicted_ac_power = model(X_test_tensor)  # Model predictions\n",
    "    actual_ac_power = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "    # Absolute residuals between predicted and actual AC power\n",
    "    residuals = torch.abs(predicted_ac_power - actual_ac_power)\n",
    "\n",
    "# Calculated the standard deviation for setting the fault threshold\n",
    "sigma = torch.std(residuals).item()\n",
    "threshold = 3 * sigma  # Set the threshold as 3 * sigma\n",
    "\n",
    "anomalies = (residuals > threshold).int()\n",
    "\n",
    "fault_indices = torch.nonzero(anomalies).squeeze().tolist()\n",
    "\n",
    "check_consecutive_faults(fault_indices, df, window=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fa7b8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b122f937",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, roc_curve, precision_recall_curve, accuracy_score, classification_report, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "126ea073",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r'C:\\Users\\My PC\\Desktop\\Solar PV Fault Research\\Seasonal_Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8fa0393",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Season-2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9a85a78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Inv</th>\n",
       "      <th>AC_Real_Power</th>\n",
       "      <th>AC_Current</th>\n",
       "      <th>DC_Power</th>\n",
       "      <th>DC_Current</th>\n",
       "      <th>Tilt_Irradiation_1</th>\n",
       "      <th>Temp_Ambient_1</th>\n",
       "      <th>Temp_Module_1</th>\n",
       "      <th>Wind_Speed_1</th>\n",
       "      <th>Hor_Irradiation_1</th>\n",
       "      <th>Operation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-04-01</td>\n",
       "      <td>05:45:00</td>\n",
       "      <td>Inv-1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.7</td>\n",
       "      <td>19.6</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-04-01</td>\n",
       "      <td>06:00:00</td>\n",
       "      <td>Inv-1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.90</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>20.1</td>\n",
       "      <td>0.535</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2019-04-01</td>\n",
       "      <td>06:15:00</td>\n",
       "      <td>Inv-1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>0.500</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2019-04-01</td>\n",
       "      <td>06:30:00</td>\n",
       "      <td>Inv-1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.80</td>\n",
       "      <td>7.2</td>\n",
       "      <td>19.0</td>\n",
       "      <td>20.6</td>\n",
       "      <td>0.510</td>\n",
       "      <td>10.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2019-04-01</td>\n",
       "      <td>06:45:00</td>\n",
       "      <td>Inv-1</td>\n",
       "      <td>1.27</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.57</td>\n",
       "      <td>5.20</td>\n",
       "      <td>25.9</td>\n",
       "      <td>19.7</td>\n",
       "      <td>21.1</td>\n",
       "      <td>0.569</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41931</th>\n",
       "      <td>14012</td>\n",
       "      <td>14012</td>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>18:30:00</td>\n",
       "      <td>Inv-16</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.9</td>\n",
       "      <td>17.5</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41932</th>\n",
       "      <td>14013</td>\n",
       "      <td>14013</td>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>18:45:00</td>\n",
       "      <td>Inv-16</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.1</td>\n",
       "      <td>16.7</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41933</th>\n",
       "      <td>14014</td>\n",
       "      <td>14014</td>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>19:00:00</td>\n",
       "      <td>Inv-16</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.88</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.2</td>\n",
       "      <td>16.1</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41934</th>\n",
       "      <td>14015</td>\n",
       "      <td>14015</td>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>19:15:00</td>\n",
       "      <td>Inv-16</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.89</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.3</td>\n",
       "      <td>15.9</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41935</th>\n",
       "      <td>14016</td>\n",
       "      <td>14016</td>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>19:30:00</td>\n",
       "      <td>Inv-16</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.98</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.2</td>\n",
       "      <td>15.6</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41936 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0.1  Unnamed: 0        Date      Time     Inv  AC_Real_Power  \\\n",
       "0                 0           0  2019-04-01  05:45:00   Inv-1           0.00   \n",
       "1                 1           1  2019-04-01  06:00:00   Inv-1           0.00   \n",
       "2                 2           2  2019-04-01  06:15:00   Inv-1           0.00   \n",
       "3                 3           3  2019-04-01  06:30:00   Inv-1           0.00   \n",
       "4                 4           4  2019-04-01  06:45:00   Inv-1           1.27   \n",
       "...             ...         ...         ...       ...     ...            ...   \n",
       "41931         14012       14012  2020-04-01  18:30:00  Inv-16           0.00   \n",
       "41932         14013       14013  2020-04-01  18:45:00  Inv-16           0.00   \n",
       "41933         14014       14014  2020-04-01  19:00:00  Inv-16           0.00   \n",
       "41934         14015       14015  2020-04-01  19:15:00  Inv-16           0.00   \n",
       "41935         14016       14016  2020-04-01  19:30:00  Inv-16           0.00   \n",
       "\n",
       "       AC_Current  DC_Power  DC_Current  Tilt_Irradiation_1  Temp_Ambient_1  \\\n",
       "0             0.0      0.00        5.00                 0.0            17.7   \n",
       "1             0.0      0.00        4.90                 0.0            19.2   \n",
       "2             0.0      0.00        0.61                 0.0            19.0   \n",
       "3             0.0      0.00        0.80                 7.2            19.0   \n",
       "4             6.0      1.57        5.20                25.9            19.7   \n",
       "...           ...       ...         ...                 ...             ...   \n",
       "41931         0.0      0.00        0.12                 0.0            15.9   \n",
       "41932         0.0      0.00        1.24                 0.0            15.1   \n",
       "41933         0.0      0.00        2.88                 0.0            14.2   \n",
       "41934         0.0      0.00        2.89                 0.0            14.3   \n",
       "41935         0.0      0.00        2.98                 0.0            14.2   \n",
       "\n",
       "       Temp_Module_1  Wind_Speed_1  Hor_Irradiation_1  Operation  \n",
       "0               19.6         0.500                0.0          1  \n",
       "1               20.1         0.535                0.0          1  \n",
       "2               20.1         0.500                1.6          1  \n",
       "3               20.6         0.510               10.4          1  \n",
       "4               21.1         0.569               27.0          1  \n",
       "...              ...           ...                ...        ...  \n",
       "41931           17.5         0.500                0.5          1  \n",
       "41932           16.7         0.500                0.5          1  \n",
       "41933           16.1         0.500                0.5          1  \n",
       "41934           15.9         0.500                0.5          1  \n",
       "41935           15.6         0.500                0.5          1  \n",
       "\n",
       "[41936 rows x 15 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cd297e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 41936 entries, 0 to 41935\n",
      "Data columns (total 15 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   Unnamed: 0.1        41936 non-null  int64  \n",
      " 1   Unnamed: 0          41936 non-null  int64  \n",
      " 2   Date                41936 non-null  object \n",
      " 3   Time                41936 non-null  object \n",
      " 4   Inv                 41936 non-null  object \n",
      " 5   AC_Real_Power       41936 non-null  float64\n",
      " 6   AC_Current          41936 non-null  float64\n",
      " 7   DC_Power            41936 non-null  float64\n",
      " 8   DC_Current          41936 non-null  float64\n",
      " 9   Tilt_Irradiation_1  41936 non-null  float64\n",
      " 10  Temp_Ambient_1      41936 non-null  float64\n",
      " 11  Temp_Module_1       41936 non-null  float64\n",
      " 12  Wind_Speed_1        41936 non-null  float64\n",
      " 13  Hor_Irradiation_1   41936 non-null  float64\n",
      " 14  Operation           41936 non-null  int64  \n",
      "dtypes: float64(9), int64(3), object(3)\n",
      "memory usage: 4.8+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Inv</th>\n",
       "      <th>AC_Real_Power</th>\n",
       "      <th>AC_Current</th>\n",
       "      <th>DC_Power</th>\n",
       "      <th>DC_Current</th>\n",
       "      <th>Tilt_Irradiation_1</th>\n",
       "      <th>Temp_Ambient_1</th>\n",
       "      <th>Temp_Module_1</th>\n",
       "      <th>Wind_Speed_1</th>\n",
       "      <th>Hor_Irradiation_1</th>\n",
       "      <th>Operation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-04-01</td>\n",
       "      <td>05:45:00</td>\n",
       "      <td>Inv-1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.7</td>\n",
       "      <td>19.6</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-04-01</td>\n",
       "      <td>06:00:00</td>\n",
       "      <td>Inv-1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.90</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>20.1</td>\n",
       "      <td>0.535</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2019-04-01</td>\n",
       "      <td>06:15:00</td>\n",
       "      <td>Inv-1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>0.500</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2019-04-01</td>\n",
       "      <td>06:30:00</td>\n",
       "      <td>Inv-1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.80</td>\n",
       "      <td>7.2</td>\n",
       "      <td>19.0</td>\n",
       "      <td>20.6</td>\n",
       "      <td>0.510</td>\n",
       "      <td>10.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2019-04-01</td>\n",
       "      <td>06:45:00</td>\n",
       "      <td>Inv-1</td>\n",
       "      <td>1.27</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.57</td>\n",
       "      <td>5.20</td>\n",
       "      <td>25.9</td>\n",
       "      <td>19.7</td>\n",
       "      <td>21.1</td>\n",
       "      <td>0.569</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0        Date      Time    Inv  AC_Real_Power  \\\n",
       "0             0           0  2019-04-01  05:45:00  Inv-1           0.00   \n",
       "1             1           1  2019-04-01  06:00:00  Inv-1           0.00   \n",
       "2             2           2  2019-04-01  06:15:00  Inv-1           0.00   \n",
       "3             3           3  2019-04-01  06:30:00  Inv-1           0.00   \n",
       "4             4           4  2019-04-01  06:45:00  Inv-1           1.27   \n",
       "\n",
       "   AC_Current  DC_Power  DC_Current  Tilt_Irradiation_1  Temp_Ambient_1  \\\n",
       "0         0.0      0.00        5.00                 0.0            17.7   \n",
       "1         0.0      0.00        4.90                 0.0            19.2   \n",
       "2         0.0      0.00        0.61                 0.0            19.0   \n",
       "3         0.0      0.00        0.80                 7.2            19.0   \n",
       "4         6.0      1.57        5.20                25.9            19.7   \n",
       "\n",
       "   Temp_Module_1  Wind_Speed_1  Hor_Irradiation_1  Operation  \n",
       "0           19.6         0.500                0.0          1  \n",
       "1           20.1         0.535                0.0          1  \n",
       "2           20.1         0.500                1.6          1  \n",
       "3           20.6         0.510               10.4          1  \n",
       "4           21.1         0.569               27.0          1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df.info(),df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da140a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 41936\n",
      "Number of columns: 15\n"
     ]
    }
   ],
   "source": [
    "num_rows, num_columns = df.shape\n",
    "\n",
    "print(\"Number of rows:\", num_rows)\n",
    "print(\"Number of columns:\", num_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "301d1376",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['Unnamed: 0.1', 'Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a94f428",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DateTime'] = pd.to_datetime(df['Date'] + ' ' + df['Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e26e412b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('DateTime', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32668783",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Date','Time'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d63c420c",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = pd.Timestamp(\"07:00:00\")\n",
    "end_time = pd.Timestamp(\"18:30:00\")\n",
    "\n",
    "# Filtering the data to retain only the rows within the operational hours\n",
    "df = df[(df.index.time >= pd.to_datetime('7:00:00').time()) & (df.index.time <= pd.to_datetime('18:30:00').time())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "354e4772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inv                   0\n",
      "AC_Real_Power         0\n",
      "AC_Current            0\n",
      "DC_Power              0\n",
      "DC_Current            0\n",
      "Tilt_Irradiation_1    0\n",
      "Temp_Ambient_1        0\n",
      "Temp_Module_1         0\n",
      "Wind_Speed_1          0\n",
      "Hor_Irradiation_1     0\n",
      "Operation             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "missing_data = df.isnull().sum()\n",
    "print(missing_data)\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "957d0f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "operational_data = df\n",
    "numerical_features = ['AC_Real_Power','Tilt_Irradiation_1', 'Temp_Ambient_1', 'Hor_Irradiation_1']\n",
    "\n",
    "# Normalization of the features\n",
    "operational_data[numerical_features] = scaler.fit_transform(operational_data[numerical_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93bc4b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = operational_data[['Tilt_Irradiation_1', 'Temp_Ambient_1', 'Hor_Irradiation_1']]\n",
    "y = operational_data['AC_Real_Power']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed533d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training - Epoch [1/200], Loss: 0.0800\n",
      "Validation - Epoch [1/200], Loss: 0.0123\n",
      "Training - Epoch [2/200], Loss: 0.0139\n",
      "Validation - Epoch [2/200], Loss: 0.0134\n",
      "Training - Epoch [3/200], Loss: 0.0135\n",
      "Validation - Epoch [3/200], Loss: 0.0127\n",
      "Training - Epoch [4/200], Loss: 0.0133\n",
      "Validation - Epoch [4/200], Loss: 0.0165\n",
      "Training - Epoch [5/200], Loss: 0.0131\n",
      "Validation - Epoch [5/200], Loss: 0.0151\n",
      "Training - Epoch [6/200], Loss: 0.0131\n",
      "Validation - Epoch [6/200], Loss: 0.0130\n",
      "Training - Epoch [7/200], Loss: 0.0130\n",
      "Validation - Epoch [7/200], Loss: 0.0129\n",
      "Training - Epoch [8/200], Loss: 0.0130\n",
      "Validation - Epoch [8/200], Loss: 0.0124\n",
      "Training - Epoch [9/200], Loss: 0.0128\n",
      "Validation - Epoch [9/200], Loss: 0.0133\n",
      "Training - Epoch [10/200], Loss: 0.0129\n",
      "Validation - Epoch [10/200], Loss: 0.0130\n",
      "Training - Epoch [11/200], Loss: 0.0130\n",
      "Validation - Epoch [11/200], Loss: 0.0133\n",
      "Training - Epoch [12/200], Loss: 0.0130\n",
      "Validation - Epoch [12/200], Loss: 0.0130\n",
      "Training - Epoch [13/200], Loss: 0.0126\n",
      "Validation - Epoch [13/200], Loss: 0.0121\n",
      "Training - Epoch [14/200], Loss: 0.0128\n",
      "Validation - Epoch [14/200], Loss: 0.0124\n",
      "Training - Epoch [15/200], Loss: 0.0130\n",
      "Validation - Epoch [15/200], Loss: 0.0140\n",
      "Training - Epoch [16/200], Loss: 0.0125\n",
      "Validation - Epoch [16/200], Loss: 0.0123\n",
      "Training - Epoch [17/200], Loss: 0.0125\n",
      "Validation - Epoch [17/200], Loss: 0.0129\n",
      "Training - Epoch [18/200], Loss: 0.0124\n",
      "Validation - Epoch [18/200], Loss: 0.0119\n",
      "Training - Epoch [19/200], Loss: 0.0125\n",
      "Validation - Epoch [19/200], Loss: 0.0139\n",
      "Training - Epoch [20/200], Loss: 0.0125\n",
      "Validation - Epoch [20/200], Loss: 0.0120\n",
      "Training - Epoch [21/200], Loss: 0.0125\n",
      "Validation - Epoch [21/200], Loss: 0.0122\n",
      "Training - Epoch [22/200], Loss: 0.0123\n",
      "Validation - Epoch [22/200], Loss: 0.0121\n",
      "Training - Epoch [23/200], Loss: 0.0126\n",
      "Validation - Epoch [23/200], Loss: 0.0119\n",
      "Training - Epoch [24/200], Loss: 0.0122\n",
      "Validation - Epoch [24/200], Loss: 0.0122\n",
      "Training - Epoch [25/200], Loss: 0.0122\n",
      "Validation - Epoch [25/200], Loss: 0.0122\n",
      "Training - Epoch [26/200], Loss: 0.0124\n",
      "Validation - Epoch [26/200], Loss: 0.0135\n",
      "Training - Epoch [27/200], Loss: 0.0122\n",
      "Validation - Epoch [27/200], Loss: 0.0118\n",
      "Training - Epoch [28/200], Loss: 0.0176\n",
      "Validation - Epoch [28/200], Loss: 0.0143\n",
      "Training - Epoch [29/200], Loss: 0.0133\n",
      "Validation - Epoch [29/200], Loss: 0.0136\n",
      "Training - Epoch [30/200], Loss: 0.0132\n",
      "Validation - Epoch [30/200], Loss: 0.0129\n",
      "Training - Epoch [31/200], Loss: 0.0138\n",
      "Validation - Epoch [31/200], Loss: 0.0133\n",
      "Training - Epoch [32/200], Loss: 0.0132\n",
      "Validation - Epoch [32/200], Loss: 0.0136\n",
      "Training - Epoch [33/200], Loss: 0.0131\n",
      "Validation - Epoch [33/200], Loss: 0.0127\n",
      "Training - Epoch [34/200], Loss: 0.0131\n",
      "Validation - Epoch [34/200], Loss: 0.0125\n",
      "Training - Epoch [35/200], Loss: 0.0136\n",
      "Validation - Epoch [35/200], Loss: 0.0132\n",
      "Training - Epoch [36/200], Loss: 0.0133\n",
      "Validation - Epoch [36/200], Loss: 0.0138\n",
      "Training - Epoch [37/200], Loss: 0.0129\n",
      "Validation - Epoch [37/200], Loss: 0.0133\n",
      "Training - Epoch [38/200], Loss: 0.0130\n",
      "Validation - Epoch [38/200], Loss: 0.0128\n",
      "Training - Epoch [39/200], Loss: 0.0130\n",
      "Validation - Epoch [39/200], Loss: 0.0140\n",
      "Training - Epoch [40/200], Loss: 0.0129\n",
      "Validation - Epoch [40/200], Loss: 0.0131\n",
      "Training - Epoch [41/200], Loss: 0.0129\n",
      "Validation - Epoch [41/200], Loss: 0.0127\n",
      "Training - Epoch [42/200], Loss: 0.0127\n",
      "Validation - Epoch [42/200], Loss: 0.0136\n",
      "Training - Epoch [43/200], Loss: 0.0128\n",
      "Validation - Epoch [43/200], Loss: 0.0126\n",
      "Training - Epoch [44/200], Loss: 0.0126\n",
      "Validation - Epoch [44/200], Loss: 0.0125\n",
      "Training - Epoch [45/200], Loss: 0.0130\n",
      "Validation - Epoch [45/200], Loss: 0.0127\n",
      "Training - Epoch [46/200], Loss: 0.0126\n",
      "Validation - Epoch [46/200], Loss: 0.0126\n",
      "Training - Epoch [47/200], Loss: 0.0125\n",
      "Validation - Epoch [47/200], Loss: 0.0127\n",
      "Training - Epoch [48/200], Loss: 0.0126\n",
      "Validation - Epoch [48/200], Loss: 0.0126\n",
      "Training - Epoch [49/200], Loss: 0.0128\n",
      "Validation - Epoch [49/200], Loss: 0.0135\n",
      "Training - Epoch [50/200], Loss: 0.0136\n",
      "Validation - Epoch [50/200], Loss: 0.0125\n",
      "Training - Epoch [51/200], Loss: 0.0125\n",
      "Validation - Epoch [51/200], Loss: 0.0124\n",
      "Training - Epoch [52/200], Loss: 0.0126\n",
      "Validation - Epoch [52/200], Loss: 0.0126\n",
      "Training - Epoch [53/200], Loss: 0.0125\n",
      "Validation - Epoch [53/200], Loss: 0.0131\n",
      "Training - Epoch [54/200], Loss: 0.0125\n",
      "Validation - Epoch [54/200], Loss: 0.0127\n",
      "Training - Epoch [55/200], Loss: 0.0125\n",
      "Validation - Epoch [55/200], Loss: 0.0130\n",
      "Training - Epoch [56/200], Loss: 0.0126\n",
      "Validation - Epoch [56/200], Loss: 0.0124\n",
      "Training - Epoch [57/200], Loss: 0.0125\n",
      "Validation - Epoch [57/200], Loss: 0.0125\n",
      "Training - Epoch [58/200], Loss: 0.0125\n",
      "Validation - Epoch [58/200], Loss: 0.0124\n",
      "Training - Epoch [59/200], Loss: 0.0126\n",
      "Validation - Epoch [59/200], Loss: 0.0128\n",
      "Training - Epoch [60/200], Loss: 0.0125\n",
      "Validation - Epoch [60/200], Loss: 0.0123\n",
      "Training - Epoch [61/200], Loss: 0.0128\n",
      "Validation - Epoch [61/200], Loss: 0.0129\n",
      "Training - Epoch [62/200], Loss: 0.0125\n",
      "Validation - Epoch [62/200], Loss: 0.0129\n",
      "Training - Epoch [63/200], Loss: 0.0124\n",
      "Validation - Epoch [63/200], Loss: 0.0124\n",
      "Training - Epoch [64/200], Loss: 0.0125\n",
      "Validation - Epoch [64/200], Loss: 0.0124\n",
      "Training - Epoch [65/200], Loss: 0.0124\n",
      "Validation - Epoch [65/200], Loss: 0.0130\n",
      "Training - Epoch [66/200], Loss: 0.0125\n",
      "Validation - Epoch [66/200], Loss: 0.0126\n",
      "Training - Epoch [67/200], Loss: 0.0125\n",
      "Validation - Epoch [67/200], Loss: 0.0122\n",
      "Training - Epoch [68/200], Loss: 0.0124\n",
      "Validation - Epoch [68/200], Loss: 0.0127\n",
      "Training - Epoch [69/200], Loss: 0.0141\n",
      "Validation - Epoch [69/200], Loss: 0.0192\n",
      "Training - Epoch [70/200], Loss: 0.0130\n",
      "Validation - Epoch [70/200], Loss: 0.0129\n",
      "Training - Epoch [71/200], Loss: 0.0125\n",
      "Validation - Epoch [71/200], Loss: 0.0124\n",
      "Training - Epoch [72/200], Loss: 0.0124\n",
      "Validation - Epoch [72/200], Loss: 0.0131\n",
      "Training - Epoch [73/200], Loss: 0.0124\n",
      "Validation - Epoch [73/200], Loss: 0.0134\n",
      "Training - Epoch [74/200], Loss: 0.0125\n",
      "Validation - Epoch [74/200], Loss: 0.0122\n",
      "Training - Epoch [75/200], Loss: 0.0128\n",
      "Validation - Epoch [75/200], Loss: 0.0140\n",
      "Training - Epoch [76/200], Loss: 0.0129\n",
      "Validation - Epoch [76/200], Loss: 0.0125\n",
      "Training - Epoch [77/200], Loss: 0.0123\n",
      "Validation - Epoch [77/200], Loss: 0.0123\n",
      "Training - Epoch [78/200], Loss: 0.0123\n",
      "Validation - Epoch [78/200], Loss: 0.0138\n",
      "Training - Epoch [79/200], Loss: 0.0124\n",
      "Validation - Epoch [79/200], Loss: 0.0127\n",
      "Training - Epoch [80/200], Loss: 0.0125\n",
      "Validation - Epoch [80/200], Loss: 0.0129\n",
      "Training - Epoch [81/200], Loss: 0.0124\n",
      "Validation - Epoch [81/200], Loss: 0.0122\n",
      "Training - Epoch [82/200], Loss: 0.0123\n",
      "Validation - Epoch [82/200], Loss: 0.0124\n",
      "Training - Epoch [83/200], Loss: 0.0125\n",
      "Validation - Epoch [83/200], Loss: 0.0129\n",
      "Training - Epoch [84/200], Loss: 0.0125\n",
      "Validation - Epoch [84/200], Loss: 0.0127\n",
      "Training - Epoch [85/200], Loss: 0.0126\n",
      "Validation - Epoch [85/200], Loss: 0.0125\n",
      "Training - Epoch [86/200], Loss: 0.0126\n",
      "Validation - Epoch [86/200], Loss: 0.0122\n",
      "Training - Epoch [87/200], Loss: 0.0125\n",
      "Validation - Epoch [87/200], Loss: 0.0125\n",
      "Training - Epoch [88/200], Loss: 0.0124\n",
      "Validation - Epoch [88/200], Loss: 0.0126\n",
      "Training - Epoch [89/200], Loss: 0.0123\n",
      "Validation - Epoch [89/200], Loss: 0.0123\n",
      "Training - Epoch [90/200], Loss: 0.0123\n",
      "Validation - Epoch [90/200], Loss: 0.0127\n",
      "Training - Epoch [91/200], Loss: 0.0123\n",
      "Validation - Epoch [91/200], Loss: 0.0134\n",
      "Training - Epoch [92/200], Loss: 0.0123\n",
      "Validation - Epoch [92/200], Loss: 0.0124\n",
      "Training - Epoch [93/200], Loss: 0.0122\n",
      "Validation - Epoch [93/200], Loss: 0.0124\n",
      "Training - Epoch [94/200], Loss: 0.0122\n",
      "Validation - Epoch [94/200], Loss: 0.0122\n",
      "Training - Epoch [95/200], Loss: 0.0125\n",
      "Validation - Epoch [95/200], Loss: 0.0134\n",
      "Training - Epoch [96/200], Loss: 0.0124\n",
      "Validation - Epoch [96/200], Loss: 0.0124\n",
      "Training - Epoch [97/200], Loss: 0.0122\n",
      "Validation - Epoch [97/200], Loss: 0.0123\n",
      "Training - Epoch [98/200], Loss: 0.0123\n",
      "Validation - Epoch [98/200], Loss: 0.0129\n",
      "Training - Epoch [99/200], Loss: 0.0123\n",
      "Validation - Epoch [99/200], Loss: 0.0125\n",
      "Training - Epoch [100/200], Loss: 0.0123\n",
      "Validation - Epoch [100/200], Loss: 0.0121\n",
      "Training - Epoch [101/200], Loss: 0.0123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Epoch [101/200], Loss: 0.0129\n",
      "Training - Epoch [102/200], Loss: 0.0122\n",
      "Validation - Epoch [102/200], Loss: 0.0136\n",
      "Training - Epoch [103/200], Loss: 0.0124\n",
      "Validation - Epoch [103/200], Loss: 0.0128\n",
      "Training - Epoch [104/200], Loss: 0.0122\n",
      "Validation - Epoch [104/200], Loss: 0.0127\n",
      "Training - Epoch [105/200], Loss: 0.0127\n",
      "Validation - Epoch [105/200], Loss: 0.0134\n",
      "Training - Epoch [106/200], Loss: 0.0125\n",
      "Validation - Epoch [106/200], Loss: 0.0128\n",
      "Training - Epoch [107/200], Loss: 0.0126\n",
      "Validation - Epoch [107/200], Loss: 0.0135\n",
      "Training - Epoch [108/200], Loss: 0.0125\n",
      "Validation - Epoch [108/200], Loss: 0.0124\n",
      "Training - Epoch [109/200], Loss: 0.0125\n",
      "Validation - Epoch [109/200], Loss: 0.0129\n",
      "Training - Epoch [110/200], Loss: 0.0122\n",
      "Validation - Epoch [110/200], Loss: 0.0121\n",
      "Training - Epoch [111/200], Loss: 0.0122\n",
      "Validation - Epoch [111/200], Loss: 0.0129\n",
      "Training - Epoch [112/200], Loss: 0.0122\n",
      "Validation - Epoch [112/200], Loss: 0.0126\n",
      "Training - Epoch [113/200], Loss: 0.0125\n",
      "Validation - Epoch [113/200], Loss: 0.0126\n",
      "Training - Epoch [114/200], Loss: 0.0125\n",
      "Validation - Epoch [114/200], Loss: 0.0129\n",
      "Training - Epoch [115/200], Loss: 0.0122\n",
      "Validation - Epoch [115/200], Loss: 0.0135\n",
      "Training - Epoch [116/200], Loss: 0.0123\n",
      "Validation - Epoch [116/200], Loss: 0.0127\n",
      "Training - Epoch [117/200], Loss: 0.0123\n",
      "Validation - Epoch [117/200], Loss: 0.0126\n",
      "Training - Epoch [118/200], Loss: 0.0245\n",
      "Validation - Epoch [118/200], Loss: 0.0157\n",
      "Training - Epoch [119/200], Loss: 0.0160\n",
      "Validation - Epoch [119/200], Loss: 0.0152\n",
      "Training - Epoch [120/200], Loss: 0.0149\n",
      "Validation - Epoch [120/200], Loss: 0.0144\n",
      "Training - Epoch [121/200], Loss: 0.0142\n",
      "Validation - Epoch [121/200], Loss: 0.0132\n",
      "Training - Epoch [122/200], Loss: 0.0142\n",
      "Validation - Epoch [122/200], Loss: 0.0150\n",
      "Training - Epoch [123/200], Loss: 0.0140\n",
      "Validation - Epoch [123/200], Loss: 0.0132\n",
      "Training - Epoch [124/200], Loss: 0.0141\n",
      "Validation - Epoch [124/200], Loss: 0.0135\n",
      "Training - Epoch [125/200], Loss: 0.0139\n",
      "Validation - Epoch [125/200], Loss: 0.0135\n",
      "Training - Epoch [126/200], Loss: 0.0134\n",
      "Validation - Epoch [126/200], Loss: 0.0131\n",
      "Training - Epoch [127/200], Loss: 0.0132\n",
      "Validation - Epoch [127/200], Loss: 0.0136\n",
      "Training - Epoch [128/200], Loss: 0.0126\n",
      "Validation - Epoch [128/200], Loss: 0.0132\n",
      "Training - Epoch [129/200], Loss: 0.0130\n",
      "Validation - Epoch [129/200], Loss: 0.0137\n",
      "Training - Epoch [130/200], Loss: 0.0125\n",
      "Validation - Epoch [130/200], Loss: 0.0140\n",
      "Training - Epoch [131/200], Loss: 0.0124\n",
      "Validation - Epoch [131/200], Loss: 0.0124\n",
      "Training - Epoch [132/200], Loss: 0.0126\n",
      "Validation - Epoch [132/200], Loss: 0.0145\n",
      "Training - Epoch [133/200], Loss: 0.0124\n",
      "Validation - Epoch [133/200], Loss: 0.0123\n",
      "Training - Epoch [134/200], Loss: 0.0127\n",
      "Validation - Epoch [134/200], Loss: 0.0133\n",
      "Training - Epoch [135/200], Loss: 0.0128\n",
      "Validation - Epoch [135/200], Loss: 0.0137\n",
      "Training - Epoch [136/200], Loss: 0.0125\n",
      "Validation - Epoch [136/200], Loss: 0.0123\n",
      "Training - Epoch [137/200], Loss: 0.0123\n",
      "Validation - Epoch [137/200], Loss: 0.0121\n",
      "Training - Epoch [138/200], Loss: 0.0124\n",
      "Validation - Epoch [138/200], Loss: 0.0127\n",
      "Training - Epoch [139/200], Loss: 0.0125\n",
      "Validation - Epoch [139/200], Loss: 0.0127\n",
      "Training - Epoch [140/200], Loss: 0.0125\n",
      "Validation - Epoch [140/200], Loss: 0.0128\n",
      "Training - Epoch [141/200], Loss: 0.0123\n",
      "Validation - Epoch [141/200], Loss: 0.0128\n",
      "Training - Epoch [142/200], Loss: 0.0123\n",
      "Validation - Epoch [142/200], Loss: 0.0130\n",
      "Training - Epoch [143/200], Loss: 0.0123\n",
      "Validation - Epoch [143/200], Loss: 0.0127\n",
      "Training - Epoch [144/200], Loss: 0.0124\n",
      "Validation - Epoch [144/200], Loss: 0.0126\n",
      "Training - Epoch [145/200], Loss: 0.0123\n",
      "Validation - Epoch [145/200], Loss: 0.0122\n",
      "Training - Epoch [146/200], Loss: 0.0124\n",
      "Validation - Epoch [146/200], Loss: 0.0126\n",
      "Training - Epoch [147/200], Loss: 0.0123\n",
      "Validation - Epoch [147/200], Loss: 0.0123\n",
      "Training - Epoch [148/200], Loss: 0.0123\n",
      "Validation - Epoch [148/200], Loss: 0.0129\n",
      "Training - Epoch [149/200], Loss: 0.0124\n",
      "Validation - Epoch [149/200], Loss: 0.0137\n",
      "Training - Epoch [150/200], Loss: 0.0123\n",
      "Validation - Epoch [150/200], Loss: 0.0127\n",
      "Training - Epoch [151/200], Loss: 0.0123\n",
      "Validation - Epoch [151/200], Loss: 0.0123\n",
      "Training - Epoch [152/200], Loss: 0.0124\n",
      "Validation - Epoch [152/200], Loss: 0.0125\n",
      "Training - Epoch [153/200], Loss: 0.0123\n",
      "Validation - Epoch [153/200], Loss: 0.0129\n",
      "Training - Epoch [154/200], Loss: 0.0122\n",
      "Validation - Epoch [154/200], Loss: 0.0123\n",
      "Training - Epoch [155/200], Loss: 0.0122\n",
      "Validation - Epoch [155/200], Loss: 0.0125\n",
      "Training - Epoch [156/200], Loss: 0.0124\n",
      "Validation - Epoch [156/200], Loss: 0.0124\n",
      "Training - Epoch [157/200], Loss: 0.0123\n",
      "Validation - Epoch [157/200], Loss: 0.0129\n",
      "Training - Epoch [158/200], Loss: 0.0122\n",
      "Validation - Epoch [158/200], Loss: 0.0122\n",
      "Training - Epoch [159/200], Loss: 0.0122\n",
      "Validation - Epoch [159/200], Loss: 0.0126\n",
      "Training - Epoch [160/200], Loss: 0.0122\n",
      "Validation - Epoch [160/200], Loss: 0.0123\n",
      "Training - Epoch [161/200], Loss: 0.0122\n",
      "Validation - Epoch [161/200], Loss: 0.0127\n",
      "Training - Epoch [162/200], Loss: 0.0131\n",
      "Validation - Epoch [162/200], Loss: 0.0131\n",
      "Training - Epoch [163/200], Loss: 0.0123\n",
      "Validation - Epoch [163/200], Loss: 0.0122\n",
      "Training - Epoch [164/200], Loss: 0.0122\n",
      "Validation - Epoch [164/200], Loss: 0.0127\n",
      "Training - Epoch [165/200], Loss: 0.0122\n",
      "Validation - Epoch [165/200], Loss: 0.0131\n",
      "Training - Epoch [166/200], Loss: 0.0122\n",
      "Validation - Epoch [166/200], Loss: 0.0127\n",
      "Training - Epoch [167/200], Loss: 0.0122\n",
      "Validation - Epoch [167/200], Loss: 0.0129\n",
      "Training - Epoch [168/200], Loss: 0.0124\n",
      "Validation - Epoch [168/200], Loss: 0.0121\n",
      "Training - Epoch [169/200], Loss: 0.0122\n",
      "Validation - Epoch [169/200], Loss: 0.0122\n",
      "Training - Epoch [170/200], Loss: 0.0124\n",
      "Validation - Epoch [170/200], Loss: 0.0126\n",
      "Training - Epoch [171/200], Loss: 0.0123\n",
      "Validation - Epoch [171/200], Loss: 0.0128\n",
      "Training - Epoch [172/200], Loss: 0.0122\n",
      "Validation - Epoch [172/200], Loss: 0.0128\n",
      "Training - Epoch [173/200], Loss: 0.0121\n",
      "Validation - Epoch [173/200], Loss: 0.0125\n",
      "Training - Epoch [174/200], Loss: 0.0121\n",
      "Validation - Epoch [174/200], Loss: 0.0123\n",
      "Training - Epoch [175/200], Loss: 0.0123\n",
      "Validation - Epoch [175/200], Loss: 0.0137\n",
      "Training - Epoch [176/200], Loss: 0.0122\n",
      "Validation - Epoch [176/200], Loss: 0.0126\n",
      "Training - Epoch [177/200], Loss: 0.0126\n",
      "Validation - Epoch [177/200], Loss: 0.0130\n",
      "Training - Epoch [178/200], Loss: 0.0123\n",
      "Validation - Epoch [178/200], Loss: 0.0125\n",
      "Training - Epoch [179/200], Loss: 0.0121\n",
      "Validation - Epoch [179/200], Loss: 0.0133\n",
      "Training - Epoch [180/200], Loss: 0.0121\n",
      "Validation - Epoch [180/200], Loss: 0.0127\n",
      "Training - Epoch [181/200], Loss: 0.0122\n",
      "Validation - Epoch [181/200], Loss: 0.0131\n",
      "Training - Epoch [182/200], Loss: 0.0122\n",
      "Validation - Epoch [182/200], Loss: 0.0130\n",
      "Training - Epoch [183/200], Loss: 0.0122\n",
      "Validation - Epoch [183/200], Loss: 0.0125\n",
      "Training - Epoch [184/200], Loss: 0.0122\n",
      "Validation - Epoch [184/200], Loss: 0.0124\n",
      "Training - Epoch [185/200], Loss: 0.0123\n",
      "Validation - Epoch [185/200], Loss: 0.0131\n",
      "Training - Epoch [186/200], Loss: 0.0124\n",
      "Validation - Epoch [186/200], Loss: 0.0124\n",
      "Training - Epoch [187/200], Loss: 0.0123\n",
      "Validation - Epoch [187/200], Loss: 0.0122\n",
      "Training - Epoch [188/200], Loss: 0.0122\n",
      "Validation - Epoch [188/200], Loss: 0.0140\n",
      "Training - Epoch [189/200], Loss: 0.0123\n",
      "Validation - Epoch [189/200], Loss: 0.0127\n",
      "Training - Epoch [190/200], Loss: 0.0122\n",
      "Validation - Epoch [190/200], Loss: 0.0124\n",
      "Training - Epoch [191/200], Loss: 0.0124\n",
      "Validation - Epoch [191/200], Loss: 0.0141\n",
      "Training - Epoch [192/200], Loss: 0.0126\n",
      "Validation - Epoch [192/200], Loss: 0.0124\n",
      "Training - Epoch [193/200], Loss: 0.0122\n",
      "Validation - Epoch [193/200], Loss: 0.0127\n",
      "Training - Epoch [194/200], Loss: 0.0121\n",
      "Validation - Epoch [194/200], Loss: 0.0127\n",
      "Training - Epoch [195/200], Loss: 0.0121\n",
      "Validation - Epoch [195/200], Loss: 0.0135\n",
      "Training - Epoch [196/200], Loss: 0.0121\n",
      "Validation - Epoch [196/200], Loss: 0.0132\n",
      "Training - Epoch [197/200], Loss: 0.0121\n",
      "Validation - Epoch [197/200], Loss: 0.0122\n",
      "Training - Epoch [198/200], Loss: 0.0126\n",
      "Validation - Epoch [198/200], Loss: 0.0125\n",
      "Training - Epoch [199/200], Loss: 0.0122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Epoch [199/200], Loss: 0.0126\n",
      "Training - Epoch [200/200], Loss: 0.0121\n",
      "Validation - Epoch [200/200], Loss: 0.0125\n"
     ]
    }
   ],
   "source": [
    "# Transformer Model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads, hidden_dim, num_layers, dropout):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, hidden_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=num_heads)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.squeeze(1)\n",
    "        x = self.fc(x)\n",
    "        return x.squeeze()\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = torch.tensor(features.values, dtype=torch.float32) \n",
    "        self.targets = torch.tensor(targets.values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.targets[idx]\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Hyperparameters\n",
    "input_dim = X_train.shape[1]  # Dimension of input features\n",
    "num_heads = 2  # Number of attention heads = 2\n",
    "hidden_dim = 200  # Hidden layers of the model = 200\n",
    "num_layers = 2  # Number of transformer layers = 2\n",
    "dropout = 0.1  # Dropout probability = 0.1\n",
    "\n",
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataset = CustomDataset(X_test, y_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "model = TransformerModel(input_dim=input_dim, num_heads=num_heads, hidden_dim=hidden_dim,\n",
    "                          num_layers=num_layers, dropout=dropout)\n",
    "criterion = nn.MSELoss()  # Mean Squared Error loss for regression task\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001) # Optimizer used is Adam and learning rate is 0.001\n",
    "\n",
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, labels in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * len(inputs)\n",
    "    average_loss = total_loss / len(train_dataset)\n",
    "    print(f'Training - Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss:.4f}')\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        for inputs, labels in test_dataloader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item() * len(inputs)\n",
    "        average_loss = total_loss / len(test_dataset)\n",
    "        print(f'Validation - Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss:.4f}')\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_dataloader:\n",
    "        outputs = model(inputs)\n",
    "        # Stored true labels and predictions\n",
    "        y_true.extend(labels.numpy())\n",
    "        y_pred.extend(outputs.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8c11b68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_dataloader:\n",
    "        outputs = model(inputs)\n",
    "        # Stored true labels and predictions\n",
    "        y_true.extend(labels.numpy())\n",
    "        y_pred.extend(outputs.numpy())\n",
    "\n",
    "# Converted y_true and y_pred to numpy arrays\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "# Computed confusion matrix\n",
    "cm = confusion_matrix(np.round(y_true), np.round(y_pred))\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80028fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.012502381\n",
      "Mean Absolute Error: 0.069294505\n",
      "Root Mean Squared Error: 0.111814044\n",
      "R-squared Score: 0.7696608377208396\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Computed Mean Squared Error\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "\n",
    "# Computed Mean Absolute Error\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "# Computed Root Mean Squared Error\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# Computed R2-Score\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"R-squared Score:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "40a384fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alarm triggered for 17 consecutive faults on 2019-04-15.\n",
      "Alarm triggered for 15 consecutive faults on 2019-04-01.\n",
      "Alarm triggered for 15 consecutive faults on 2019-05-18.\n",
      "Alarm triggered for 15 consecutive faults on 2019-04-04.\n",
      "Alarm triggered for 15 consecutive faults on 2019-05-25.\n",
      "Alarm triggered for 14 consecutive faults on 2019-05-06.\n",
      "Alarm triggered for 14 consecutive faults on 2019-04-21.\n",
      "Alarm triggered for 14 consecutive faults on 2019-04-17.\n",
      "Alarm triggered for 14 consecutive faults on 2019-04-30.\n",
      "Alarm triggered for 13 consecutive faults on 2019-04-20.\n",
      "Alarm triggered for 13 consecutive faults on 2019-05-19.\n",
      "Alarm triggered for 13 consecutive faults on 2019-04-13.\n",
      "Alarm triggered for 13 consecutive faults on 2019-04-25.\n",
      "Alarm triggered for 13 consecutive faults on 2019-05-31.\n",
      "Alarm triggered for 13 consecutive faults on 2019-04-11.\n",
      "Alarm triggered for 13 consecutive faults on 2019-05-08.\n",
      "Alarm triggered for 12 consecutive faults on 2019-04-10.\n",
      "Alarm triggered for 12 consecutive faults on 2019-04-24.\n",
      "Alarm triggered for 12 consecutive faults on 2019-05-29.\n",
      "Alarm triggered for 12 consecutive faults on 2019-05-09.\n",
      "Alarm triggered for 12 consecutive faults on 2019-04-16.\n",
      "Alarm triggered for 12 consecutive faults on 2019-05-10.\n",
      "Alarm triggered for 11 consecutive faults on 2019-05-26.\n",
      "Alarm triggered for 11 consecutive faults on 2019-05-04.\n",
      "Alarm triggered for 11 consecutive faults on 2019-05-22.\n",
      "Alarm triggered for 11 consecutive faults on 2019-04-27.\n",
      "Alarm triggered for 11 consecutive faults on 2019-04-29.\n",
      "Alarm triggered for 11 consecutive faults on 2019-04-22.\n",
      "Alarm triggered for 11 consecutive faults on 2019-04-18.\n",
      "Alarm triggered for 11 consecutive faults on 2019-04-05.\n",
      "Alarm triggered for 10 consecutive faults on 2019-05-27.\n",
      "Alarm triggered for 10 consecutive faults on 2019-04-06.\n",
      "Alarm triggered for 10 consecutive faults on 2019-04-14.\n",
      "Alarm triggered for 10 consecutive faults on 2019-04-19.\n",
      "Alarm triggered for 10 consecutive faults on 2019-05-24.\n",
      "Alarm triggered for 10 consecutive faults on 2019-04-08.\n",
      "Alarm triggered for 10 consecutive faults on 2019-05-28.\n",
      "Alarm triggered for 10 consecutive faults on 2019-05-30.\n",
      "Alarm triggered for 10 consecutive faults on 2019-04-26.\n",
      "Alarm triggered for 9 consecutive faults on 2019-05-17.\n",
      "Alarm triggered for 9 consecutive faults on 2019-04-02.\n",
      "Alarm triggered for 9 consecutive faults on 2019-04-09.\n",
      "Alarm triggered for 9 consecutive faults on 2019-04-28.\n",
      "Alarm triggered for 8 consecutive faults on 2019-04-23.\n",
      "Alarm triggered for 8 consecutive faults on 2019-04-07.\n",
      "Alarm triggered for 7 consecutive faults on 2019-04-03.\n",
      "Alarm triggered for 7 consecutive faults on 2019-04-12.\n",
      "Alarm triggered for 7 consecutive faults on 2019-05-21.\n",
      "Alarm triggered for 6 consecutive faults on 2019-05-07.\n",
      "Alarm triggered for 5 consecutive faults on 2019-05-23.\n",
      "Alarm triggered for 5 consecutive faults on 2019-05-05.\n",
      "Alarm triggered for 5 consecutive faults on 2019-05-01.\n",
      "Alarm triggered for 4 consecutive faults on 2019-05-20.\n"
     ]
    }
   ],
   "source": [
    "# Function to check for four consecutive faults in a day and trigger alarms where Threshold = 2 * sigma\n",
    "def check_consecutive_faults(fault_indices, df, window=3):\n",
    "    fault_timestamps = df.index[fault_indices]\n",
    "\n",
    "    grouped_by_day = fault_timestamps.to_series().dt.date\n",
    "\n",
    "    # Checked if there are three or more consecutive faults within the same day\n",
    "    daily_fault_count = grouped_by_day.value_counts()\n",
    "    for date, count in daily_fault_count.items():\n",
    "        if count >= window:\n",
    "            print(f\"Alarm triggered for {count} consecutive faults on {date}.\")\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "\n",
    "with torch.no_grad():\n",
    "    predicted_ac_power = model(X_test_tensor)  # Model predictions\n",
    "    actual_ac_power = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "    # Absolute residuals between predicted and actual AC power\n",
    "    residuals = torch.abs(predicted_ac_power - actual_ac_power)\n",
    "\n",
    "# Calculated the standard deviation for setting the fault threshold\n",
    "sigma = torch.std(residuals).item()\n",
    "threshold = 2 * sigma  # Set the threshold as 2 * sigma\n",
    "\n",
    "anomalies = (residuals > threshold).int()\n",
    "\n",
    "fault_indices = torch.nonzero(anomalies).squeeze().tolist()\n",
    "\n",
    "check_consecutive_faults(fault_indices, df, window=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "682e01cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alarm triggered for 10 consecutive faults on 2019-04-04.\n",
      "Alarm triggered for 10 consecutive faults on 2019-04-15.\n",
      "Alarm triggered for 9 consecutive faults on 2019-05-25.\n",
      "Alarm triggered for 8 consecutive faults on 2019-04-01.\n",
      "Alarm triggered for 8 consecutive faults on 2019-04-08.\n",
      "Alarm triggered for 8 consecutive faults on 2019-05-19.\n",
      "Alarm triggered for 8 consecutive faults on 2019-04-11.\n",
      "Alarm triggered for 8 consecutive faults on 2019-05-08.\n",
      "Alarm triggered for 7 consecutive faults on 2019-05-27.\n",
      "Alarm triggered for 7 consecutive faults on 2019-05-18.\n",
      "Alarm triggered for 7 consecutive faults on 2019-04-25.\n",
      "Alarm triggered for 6 consecutive faults on 2019-04-20.\n",
      "Alarm triggered for 6 consecutive faults on 2019-05-10.\n",
      "Alarm triggered for 6 consecutive faults on 2019-05-06.\n",
      "Alarm triggered for 6 consecutive faults on 2019-04-24.\n",
      "Alarm triggered for 6 consecutive faults on 2019-04-22.\n",
      "Alarm triggered for 6 consecutive faults on 2019-05-31.\n",
      "Alarm triggered for 6 consecutive faults on 2019-04-10.\n",
      "Alarm triggered for 6 consecutive faults on 2019-04-13.\n",
      "Alarm triggered for 6 consecutive faults on 2019-04-16.\n",
      "Alarm triggered for 5 consecutive faults on 2019-04-09.\n",
      "Alarm triggered for 5 consecutive faults on 2019-05-22.\n",
      "Alarm triggered for 5 consecutive faults on 2019-05-04.\n",
      "Alarm triggered for 5 consecutive faults on 2019-04-30.\n",
      "Alarm triggered for 5 consecutive faults on 2019-04-02.\n",
      "Alarm triggered for 5 consecutive faults on 2019-04-28.\n",
      "Alarm triggered for 5 consecutive faults on 2019-05-28.\n",
      "Alarm triggered for 5 consecutive faults on 2019-05-29.\n",
      "Alarm triggered for 5 consecutive faults on 2019-04-17.\n",
      "Alarm triggered for 5 consecutive faults on 2019-05-30.\n",
      "Alarm triggered for 4 consecutive faults on 2019-05-26.\n",
      "Alarm triggered for 4 consecutive faults on 2019-04-05.\n",
      "Alarm triggered for 4 consecutive faults on 2019-05-23.\n",
      "Alarm triggered for 4 consecutive faults on 2019-05-21.\n",
      "Alarm triggered for 4 consecutive faults on 2019-04-18.\n",
      "Alarm triggered for 4 consecutive faults on 2019-04-21.\n",
      "Alarm triggered for 4 consecutive faults on 2019-04-19.\n",
      "Alarm triggered for 4 consecutive faults on 2019-04-03.\n",
      "Alarm triggered for 4 consecutive faults on 2019-04-29.\n",
      "Alarm triggered for 4 consecutive faults on 2019-04-27.\n"
     ]
    }
   ],
   "source": [
    "# Function to check for four consecutive faults in a day and trigger alarms where Threshold = 3 * sigma\n",
    "def check_consecutive_faults(fault_indices, df, window=3):\n",
    "    fault_timestamps = df.index[fault_indices]\n",
    "\n",
    "    grouped_by_day = fault_timestamps.to_series().dt.date\n",
    "\n",
    "    # Checked if there are three or more consecutive faults within the same day\n",
    "    daily_fault_count = grouped_by_day.value_counts()\n",
    "    for date, count in daily_fault_count.items():\n",
    "        if count >= window:\n",
    "            print(f\"Alarm triggered for {count} consecutive faults on {date}.\")\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "\n",
    "with torch.no_grad():\n",
    "    predicted_ac_power = model(X_test_tensor)  # Model predictions\n",
    "    actual_ac_power = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "    # Absolute residuals between predicted and actual AC power\n",
    "    residuals = torch.abs(predicted_ac_power - actual_ac_power)\n",
    "\n",
    "# Calculated the standard deviation for setting the fault threshold\n",
    "sigma = torch.std(residuals).item()\n",
    "threshold = 3 * sigma  # Set the threshold as 3 * sigma\n",
    "\n",
    "anomalies = (residuals > threshold).int()\n",
    "\n",
    "fault_indices = torch.nonzero(anomalies).squeeze().tolist()\n",
    "\n",
    "check_consecutive_faults(fault_indices, df, window=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cebb09e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
